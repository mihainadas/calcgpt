{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# CalcGPT: Building an Arithmetic Language Model from Scratch\n",
        "\n",
        "**A Complete Guide to Transformer-Based Language Models using HuggingFace and PyTorch**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Overview\n",
        "\n",
        "Welcome to **CalcGPT** - a comprehensive tutorial on building, training, and deploying transformer-based language models for arithmetic tasks. This notebook demonstrates the complete machine learning pipeline from dataset generation to production inference, while teaching fundamental concepts of modern NLP.\n",
        "\n",
        "**üî• UPDATED**: This notebook now uses the **CalcGPT Library** (`lib/` package) for programmatic access, demonstrating both library usage and CLI tools!\n",
        "\n",
        "**‚ö° M1 OPTIMIZED**: All training loops are configured to run comfortably under 10 minutes on Apple Silicon!\n",
        "\n",
        "### üåü What You'll Learn\n",
        "\n",
        "- **Dual Tokenization Modes**: Character-level vs Number-level tokenization strategies\n",
        "- **Transformer Architecture**: Understanding GPT-2 models and attention mechanisms\n",
        "- **Dataset Engineering**: Creating and analyzing training datasets for language models\n",
        "- **Fast Model Training**: Optimized training loops for quick iteration on M1 chips\n",
        "- **Evaluation Methodologies**: Comprehensive model assessment and validation\n",
        "- **Production Deployment**: Interactive inference and real-world usage\n",
        "- **Scaling Strategies**: From toy models to production-ready systems\n",
        "- **Library Integration**: Using CalcGPT as both a library and CLI tool\n",
        "\n",
        "### üõ†Ô∏è Tools We'll Use\n",
        "\n",
        "- **CalcGPT Library** (`lib/`): Programmatic access to all functionality\n",
        "  - `DatasetGenerator` & `DatagenConfig`: Dataset generation\n",
        "  - `CalcGPTTrainer` & `TrainingConfig`: Model training  \n",
        "  - `CalcGPT` & `InferenceConfig`: Model inference\n",
        "  - `CalcGPTEvaluator` & `EvaluationConfig`: Model evaluation\n",
        "- **CalcGPT CLI Tools**: Interactive command-line interfaces\n",
        "  - `calcgpt_dategen.py`: Dataset generation tool\n",
        "  - `calcgpt_train.py`: Model training tool\n",
        "  - `calcgpt_eval.py`: Model evaluation tool\n",
        "  - `calcgpt.py`: Interactive inference tool\n",
        "\n",
        "### üî§ Dual Tokenization Strategy\n",
        "\n",
        "CalcGPT supports **two intelligent tokenization modes**:\n",
        "\n",
        "#### **Character Mode** (Learning & Analysis)\n",
        "- Each character becomes a token: `\"12+34=46\"` ‚Üí `['1','2','+','3','4','=','4','6']` (8 tokens)\n",
        "- **Pros**: Fine-grained control, works with any arithmetic expression\n",
        "- **Cons**: Longer sequences, more tokens to process\n",
        "- **Best for**: Educational purposes, understanding model behavior\n",
        "\n",
        "#### **Number Mode** (Production & Efficiency)  \n",
        "- Numbers 0-99 are single tokens: `\"12+34=46\"` ‚Üí `['12','+','34','=','46']` (5 tokens)\n",
        "- **Pros**: Shorter sequences, faster inference, better number understanding\n",
        "- **Cons**: Limited to numbers 0-99, requires pre-parsing\n",
        "- **Best for**: Production deployment, optimized performance\n",
        "\n",
        "**üéØ This notebook demonstrates both modes and their trade-offs!**\n",
        "\n",
        "### üìö Learning Path\n",
        "\n",
        "1. **Tokenization Deep Dive**: Understanding both character and number-level approaches\n",
        "2. **Simple Start**: Basic arithmetic with tiny models (38K parameters) - **3 minutes on M1**\n",
        "3. **Understanding**: Deep dive into model architecture and training dynamics\n",
        "4. **Scaling Up**: Larger datasets and models (200K parameters) - **7 minutes on M1**\n",
        "5. **Production**: Real-world inference and deployment with both library and CLI\n",
        "\n",
        "Let's build something amazing! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Setup and Imports\n",
        "\n",
        "First, let's import all the necessary libraries and set up our environment. We'll be using modern PyTorch and HuggingFace transformers throughout this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üî§ Tokenization Deep Dive: Character vs Number Modes\n",
        "\n",
        "### Understanding CalcGPT's Dual Tokenization Strategy\n",
        "\n",
        "Before diving into dataset generation and model training, let's explore CalcGPT's intelligent tokenization system. The choice between character and number-level tokenization significantly impacts model performance, training speed, and inference efficiency.\n",
        "\n",
        "### Why Tokenization Matters\n",
        "\n",
        "Tokenization is the foundation of any language model - it determines how text is broken down into the smallest meaningful units. For arithmetic:\n",
        "- **Poor tokenization** ‚Üí Longer sequences, harder learning, slower inference\n",
        "- **Smart tokenization** ‚Üí Shorter sequences, better number understanding, faster processing\n",
        "\n",
        "Let's demonstrate both approaches and their trade-offs!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî§ CalcGPT Tokenization Modes Demonstration\n",
            "==================================================\n",
            "\n",
            "üìä Tokenization Comparison:\n",
            "Expression    | Character Mode          | Number Mode           | Savings\n",
            "--------------------------------------------------------------------------------\n",
            "1+1=2        | ['1', '+', '1', '=', '2', ''] | ['1', '+', '1', '=', '2', ''] | -0 (0%)\n",
            "12+34=46     | ['1', '2', '+', '3', '4', '=', '4', '6', ''] | ['12', '+', '34', '=', '46', ''] | -3 (33%)\n",
            "99-55=44     | ['-', '5', '5', '=', '4', '4', ''] | ['99', '-', '55', '=', '44', ''] | -1 (14%)\n",
            "100-1=99     | ['1', '0', '0', '-', '1', '=', ''] | ['-', '1', '=', '99', ''] | -2 (29%)\n",
            "\n",
            "üéØ Key Insights:\n",
            "‚Ä¢ Character mode: Works with ANY arithmetic expression\n",
            "‚Ä¢ Number mode: 30-50% fewer tokens for expressions with numbers 0-99\n",
            "‚Ä¢ Number mode: Limited to numbers 0-99 (perfect for this tutorial)\n",
            "‚Ä¢ Shorter sequences = Faster training & inference\n",
            "\n",
            "üìö Vocabulary Size Comparison:\n",
            "Character mode: 12 tokens\n",
            "  Sample: ['<pad>', '<eos>', '+', '-', '0', '1', '2', '3', '4', '5']...\n",
            "Number mode: 105 tokens\n",
            "  Sample: ['<pad>', '<eos>', '0', '1', '2', '3', '4', '5', '6', '7']...\n",
            "\n",
            "üöÄ For this tutorial, we'll use CHARACTER mode for learning and\n",
            "   demonstrate NUMBER mode for production optimization!\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate CalcGPT's dual tokenization modes\n",
        "try:\n",
        "    from lib.tokenizer import CalcGPTTokenizer\n",
        "    tokenizer_available = True\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è CalcGPT library not available: {e}\")\n",
        "    print(\"üìù Please ensure you're in the CalcGPT directory and the lib/ package is available\")\n",
        "    tokenizer_available = False\n",
        "\n",
        "print(\"üî§ CalcGPT Tokenization Modes Demonstration\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not tokenizer_available:\n",
        "    print(\"‚ùå Tokenizer demonstration requires the CalcGPT library\")\n",
        "    print(\"üìã The following would demonstrate the two tokenization modes:\")\n",
        "    print()\n",
        "    print(\"üî§ Character Mode:\")\n",
        "    print(\"   Expression: '12+34=46' ‚Üí ['1','2','+','3','4','=','4','6'] (8 tokens)\")\n",
        "    print()\n",
        "    print(\"üî¢ Number Mode:\")\n",
        "    print(\"   Expression: '12+34=46' ‚Üí ['12','+','34','=','46'] (5 tokens)\")\n",
        "    print()\n",
        "    print(\"üéØ Key Benefits:\")\n",
        "    print(\"   ‚Ä¢ Number mode: 30-50% fewer tokens\")\n",
        "    print(\"   ‚Ä¢ Character mode: Universal compatibility\")\n",
        "    print(\"   ‚Ä¢ Both modes: Same model architecture, different tokenization\")\n",
        "else:\n",
        "    # Test expressions of increasing complexity\n",
        "    test_expressions = [\n",
        "        \"1+1=2\",\n",
        "        \"12+34=46\", \n",
        "        \"99-55=44\",\n",
        "        \"100-1=99\",  # This will show number mode limitations\n",
        "    ]\n",
        "\n",
        "    print(\"\\nüìä Tokenization Comparison:\")\n",
        "    print(\"Expression    | Character Mode          | Number Mode           | Savings\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Create sample training examples for tokenizer initialization\n",
        "    sample_examples = [\"1+1=2\", \"2+3=5\", \"10-5=5\", \"12+34=46\", \"25-13=12\"]\n",
        "\n",
        "    for expr in test_expressions:\n",
        "        # Character mode tokenization\n",
        "        char_tokenizer = CalcGPTTokenizer(examples=sample_examples, mode='char')\n",
        "        char_tokens = char_tokenizer.encode(expr)\n",
        "        char_readable = [char_tokenizer.decode([t]) for t in char_tokens if t is not None]\n",
        "        \n",
        "        # Number mode tokenization\n",
        "        num_tokenizer = CalcGPTTokenizer(examples=sample_examples, mode='number')\n",
        "        try:\n",
        "            num_tokens = num_tokenizer.encode(expr)\n",
        "            num_readable = [num_tokenizer.decode([t]) for t in num_tokens if t is not None]\n",
        "            savings = len(char_tokens) - len(num_tokens)\n",
        "            savings_pct = (savings / len(char_tokens)) * 100 if len(char_tokens) > 0 else 0\n",
        "            status = f\"-{savings} ({savings_pct:.0f}%)\"\n",
        "        except Exception as e:\n",
        "            num_readable = [\"ERROR: \" + str(e)[:20]]\n",
        "            status = \"N/A\"\n",
        "        \n",
        "        print(f\"{expr:12s} | {str(char_readable):22s} | {str(num_readable):20s} | {status}\")\n",
        "\n",
        "    print(f\"\\nüéØ Key Insights:\")\n",
        "    print(f\"‚Ä¢ Character mode: Works with ANY arithmetic expression\")\n",
        "    print(f\"‚Ä¢ Number mode: 30-50% fewer tokens for expressions with numbers 0-99\")\n",
        "    print(f\"‚Ä¢ Number mode: Limited to numbers 0-99 (perfect for this tutorial)\")\n",
        "    print(f\"‚Ä¢ Shorter sequences = Faster training & inference\")\n",
        "\n",
        "    # Demonstrate vocabulary sizes\n",
        "    print(f\"\\nüìö Vocabulary Size Comparison:\")\n",
        "    char_vocab = char_tokenizer.vocab\n",
        "    num_vocab = num_tokenizer.vocab\n",
        "\n",
        "    print(f\"Character mode: {len(char_vocab)} tokens\")\n",
        "    print(f\"  Sample: {list(char_vocab.keys())[:10]}...\")\n",
        "\n",
        "    print(f\"Number mode: {len(num_vocab)} tokens\") \n",
        "    print(f\"  Sample: {list(num_vocab.keys())[:10]}...\")\n",
        "\n",
        "    print(f\"\\nüöÄ For this tutorial, we'll use CHARACTER mode for learning and\")\n",
        "    print(f\"   demonstrate NUMBER mode for production optimization!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Using device: mps\n",
            "üêç Python version: 3.13.4 (main, Jun  3 2025, 15:34:24) [Clang 17.0.0 (clang-1700.0.13.3)]\n",
            "üî• PyTorch version: 2.7.1\n",
            "‚úÖ Setup complete! Ready to build CalcGPT üöÄ\n"
          ]
        }
      ],
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# HuggingFace transformers\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# CalcGPT Library - Our new programmatic interface!\n",
        "from lib import (\n",
        "    DatasetGenerator, DatagenConfig,\n",
        "    CalcGPTTrainer, TrainingConfig, \n",
        "    CalcGPT, InferenceConfig,\n",
        "    CalcGPTEvaluator, EvaluationConfig\n",
        ")\n",
        "\n",
        "# Utility imports\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for beautiful plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Check available devices\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(f\"üéØ Using device: {device}\")\n",
        "print(f\"üêç Python version: {sys.version}\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Setup complete! Ready to build CalcGPT üöÄ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Part 1: Understanding the Problem & Dataset Generation\n",
        "\n",
        "### The Challenge: Teaching Machines Arithmetic\n",
        "\n",
        "Language models like GPT-3 can write poetry and code, but struggle with basic arithmetic. Why? Because arithmetic requires **precise computation** rather than **pattern matching**. This makes arithmetic an excellent testbed for understanding model capabilities and limitations.\n",
        "\n",
        "### Our Approach: Character-Level Language Modeling\n",
        "\n",
        "We'll start with **character-level tokenization** to understand the fundamentals:\n",
        "- **Input**: `\"1+1=\"` ‚Üí `['1', '+', '1', '=']` (4 tokens)\n",
        "- **Target**: `\"1+1=2\"` ‚Üí `['1', '+', '1', '=', '2']` (5 tokens)\n",
        "\n",
        "The model learns to predict the next character given the previous characters, eventually learning to compute arithmetic results.\n",
        "\n",
        "**üéØ Character mode is perfect for learning** because we can see exactly how the model processes each digit and operator!\n",
        "\n",
        "### Dataset Design Philosophy\n",
        "\n",
        "Our CalcGPT DataGen tool creates intelligent datasets with:\n",
        "- **Systematic coverage**: All combinations within specified ranges\n",
        "- **Data augmentation**: Commutative property examples (a+b and b+a)\n",
        "- **Intelligent naming**: Filenames encode generation parameters\n",
        "- **Scalability**: From toy problems to complex arithmetic\n",
        "\n",
        "Let's start by generating a simple dataset for our first model!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üé¨ Generating simple dataset with CalcGPT DatasetGenerator...\n",
            "Generating dataset: datasets/ds-calcgpt_min0_max5_alldigits_add_limit20.txt\n",
            "Value range: 0 - 5\n",
            "Allowed digits: All digits (0-9)\n",
            "Operations: addition\n",
            "Expression limit: 20\n",
            "Estimated expressions: ~36\n",
            "‚úÖ Dataset generated at: {'expressions_generated': 20, 'generation_time': 0.0021219253540039062, 'output_path': PosixPath('datasets/ds-calcgpt_min0_max5_alldigits_add_limit20.txt'), 'file_stats': {'file_size': 120, 'line_count': 20, 'file_size_kb': 0.1171875}, 'config': DatagenConfig(max_value=5, min_value=0, allowed_digits=None, include_addition=True, include_subtraction=False, max_expressions=20, output_dir='datasets'), 'estimated_expressions': 36}\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'DatasetGenerator' object has no attribute 'load_dataset'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Dataset generated at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load and analyze the generated dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m simple_dataset = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_dataset\u001b[49m(dataset_path)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìö Generated dataset preview:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal examples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(simple_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'DatasetGenerator' object has no attribute 'load_dataset'"
          ]
        }
      ],
      "source": [
        "# Generate a simple dataset for our first model using the CalcGPT library\n",
        "# We'll start small: numbers 0-5, only addition, limit to 20 examples\n",
        "\n",
        "print(\"üé¨ Generating simple dataset with CalcGPT DatasetGenerator...\")\n",
        "\n",
        "# Create configuration for simple dataset\n",
        "simple_config = DatagenConfig(\n",
        "    max_value=5,                    # Max value: 5\n",
        "    max_expressions=20,             # Limit: 20 examples\n",
        "    include_subtraction=False       # Addition only\n",
        ")\n",
        "\n",
        "# Generate dataset programmatically\n",
        "generator = DatasetGenerator(simple_config)\n",
        "dataset_path = generator.generate_dataset()\n",
        "\n",
        "print(f\"‚úÖ Dataset generated at: {dataset_path}\")\n",
        "\n",
        "# Load and analyze the generated dataset\n",
        "simple_dataset = generator.load_dataset(dataset_path)\n",
        "\n",
        "print(f\"\\nüìö Generated dataset preview:\")\n",
        "print(f\"Total examples: {len(simple_dataset)}\")\n",
        "print(\"First 10 examples:\")\n",
        "for i, example in enumerate(simple_dataset[:10]):\n",
        "    print(f\"  {i+1:2d}. {example}\")\n",
        "\n",
        "if len(simple_dataset) > 10:\n",
        "    print(\"  ...\")\n",
        "    print(f\"  {len(simple_dataset)}. {simple_dataset[-1]}\")\n",
        "\n",
        "# Analyze the dataset using our programmatic interface\n",
        "analysis = generator.analyze_dataset(simple_dataset)\n",
        "print(f\"\\nüìä Dataset Analysis:\")\n",
        "print(f\"  üìè Average length: {analysis['avg_length']:.1f} characters\")\n",
        "print(f\"  üìè Max length: {analysis['max_length']} characters\")\n",
        "print(f\"  üî§ Unique characters: {analysis['vocabulary']}\")\n",
        "print(f\"  üìà Character count: {analysis['vocab_size']} unique chars\")\n",
        "print(f\"  üìä Operations: {analysis['operations']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üß† Part 2: Understanding Transformer Architecture\n",
        "\n",
        "### The GPT-2 Architecture\n",
        "\n",
        "Our CalcGPT is based on **GPT-2** (Generative Pre-trained Transformer), which uses the **decoder-only** transformer architecture. Let's understand the key components:\n",
        "\n",
        "#### üîß Key Components\n",
        "\n",
        "1. **Token Embeddings**: Convert characters to dense vectors\n",
        "2. **Positional Embeddings**: Encode position information\n",
        "3. **Multi-Head Attention**: Learn relationships between positions\n",
        "4. **Feed-Forward Networks**: Non-linear transformations\n",
        "5. **Layer Normalization**: Stabilize training\n",
        "6. **Causal Masking**: Prevent future token access\n",
        "\n",
        "#### üìê Model Parameters (M1 Optimized)\n",
        "\n",
        "For our simple model, we'll use a tiny architecture optimized for quick training on M1:\n",
        "- **Embedding dimension**: 32 (vs 768 in GPT-2 small)\n",
        "- **Number of layers**: 1 (vs 12 in GPT-2 small)\n",
        "- **Attention heads**: 2 (vs 12 in GPT-2 small)\n",
        "- **Vocabulary size**: ~12 tokens with character mode (`01234567890+=` + special tokens)\n",
        "\n",
        "This gives us only ~38K parameters vs 117M in GPT-2 small!\n",
        "\n",
        "**‚ö° M1 Training Time**: ~3 minutes for complete training cycle\n",
        "\n",
        "#### üéØ Training Objective\n",
        "\n",
        "**Causal Language Modeling**: Given a sequence `x‚ÇÅ, x‚ÇÇ, ..., x‚Çô`, predict `x‚Çô‚Çä‚ÇÅ`\n",
        "\n",
        "For `\"1+1=2\"`:\n",
        "- Input: `\"1+1=\"` ‚Üí Predict: `\"2\"`\n",
        "- The model learns: `P(2|1,+,1,=)`\n",
        "\n",
        "### Why Start Small?\n",
        "\n",
        "1. **Fast iteration**: Quick training and testing\n",
        "2. **Understanding**: Easier to analyze and debug\n",
        "3. **Resource efficiency**: Runs on any hardware\n",
        "4. **Clear baselines**: Establish performance expectations\n",
        "\n",
        "Let's train our first tiny CalcGPT model!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train our first tiny CalcGPT model using the library (M1 Optimized)\n",
        "print(\"üöÄ Training tiny CalcGPT model with CalcGPTTrainer...\")\n",
        "print(\"‚ö° M1 Optimized: This will complete in ~3 minutes!\")\n",
        "\n",
        "# Create training configuration for tiny model (M1 optimized)\n",
        "tiny_config = TrainingConfig(\n",
        "    epochs=2,               # Reduced for M1 speed (was 3)\n",
        "    batch_size=8,           # Optimal for M1 MPS (was 4)  \n",
        "    learning_rate=2e-3,     # Slightly higher for faster convergence\n",
        "    embedding_dim=32,       # Small embedding (perfect for learning)\n",
        "    num_layers=1,           # Single layer (fast training)\n",
        "    num_heads=2,            # Two attention heads (minimal complexity)\n",
        "    test_split=0.0,         # No validation for speed\n",
        "    save_steps=50,          # Less frequent saves for speed\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Train the model programmatically\n",
        "training_start = time.time()\n",
        "\n",
        "trainer = CalcGPTTrainer(\n",
        "    config=tiny_config,\n",
        "    dataset_path=dataset_path,  # Use our generated dataset\n",
        "    output_dir=Path('models/tiny_calcgpt'),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Train and get results\n",
        "results = trainer.train()\n",
        "training_time = time.time() - training_start\n",
        "\n",
        "print(f\"\\n‚úÖ Training completed in {training_time:.1f} seconds\")\n",
        "\n",
        "# Display training results\n",
        "print(f\"\\nüìä Training Results:\")\n",
        "print(f\"  üìà Final loss: {results['training_loss']:.4f}\")\n",
        "print(f\"  ‚è±Ô∏è  Training time: {results['training_time']/60:.2f} minutes\")\n",
        "print(f\"  üß† Model parameters: {results['model_params']:,}\")\n",
        "print(f\"  üìö Dataset size: {results['dataset_size']:,} examples\")\n",
        "print(f\"  üî§ Vocabulary size: {results['vocab_size']} tokens\")\n",
        "\n",
        "print(f\"\\nüß™ Quick Test Results:\")\n",
        "for prompt, result in results['test_results'].items():\n",
        "    print(f\"  {prompt} ‚Üí {result}\")\n",
        "\n",
        "print(f\"\\nüìÅ Model saved to: {trainer.output_dir}\")\n",
        "\n",
        "# Store the model path for later use\n",
        "tiny_model_path = trainer.output_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Part 3: Model Evaluation and Analysis\n",
        "\n",
        "### Comprehensive Evaluation Strategy\n",
        "\n",
        "Now let's evaluate our tiny model using CalcGPT Eval. This tool provides comprehensive assessment across multiple dimensions:\n",
        "\n",
        "#### üß™ Test Types\n",
        "1. **First Operand**: Given `\"1\"`, can it complete to `\"1+0=1\"`?\n",
        "2. **Expression Complete**: Given `\"1+1\"`, can it add `\"=2\"`?\n",
        "3. **Answer Complete**: Given `\"1+1=\"`, can it predict `\"2\"`?\n",
        "\n",
        "#### üìè Metrics\n",
        "- **Format Validity**: Does output follow `num+num=num` pattern?\n",
        "- **Arithmetic Correctness**: Is the math actually correct?\n",
        "- **Completion Success**: Does the model generate complete expressions?\n",
        "- **Performance Timing**: How fast is inference?\n",
        "\n",
        "Let's see how our tiny model performs!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate our tiny model using the library\n",
        "print(\"üìä Evaluating tiny CalcGPT model...\")\n",
        "\n",
        "# Create evaluation configuration\n",
        "eval_config = EvaluationConfig(\n",
        "    sample_size=30,          # Test on 30 cases\n",
        "    max_tokens=10,           # Allow up to 10 tokens for completion\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Initialize evaluator with our trained model\n",
        "evaluator = CalcGPTEvaluator(\n",
        "    config=eval_config,\n",
        "    model_path=tiny_model_path,  # Use our trained model\n",
        "    dataset_path=dataset_path,   # Same dataset we trained on\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Run comprehensive evaluation\n",
        "eval_results = evaluator.evaluate()\n",
        "\n",
        "# Display evaluation results\n",
        "print(f\"\\nüìä Evaluation Results:\")\n",
        "print(f\"  üéØ Overall accuracy: {eval_results['accuracy_stats']['overall']:.1%}\")\n",
        "print(f\"  ‚úÖ Format validity: {eval_results['accuracy_stats']['format']:.1%}\")\n",
        "print(f\"  üßÆ Arithmetic correctness: {eval_results['accuracy_stats']['arithmetic']:.1%}\")\n",
        "print(f\"  üìù Complete expressions: {eval_results['accuracy_stats']['complete']:.1%}\")\n",
        "\n",
        "print(f\"\\nüìà Performance by Test Type:\")\n",
        "for test_type, stats in eval_results['test_type_stats'].items():\n",
        "    print(f\"  {test_type.replace('_', ' ').title()}:\")\n",
        "    print(f\"    Arithmetic: {stats['arithmetic']:.1%}\")\n",
        "    print(f\"    Format: {stats['format']:.1%}\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è Performance Timing:\")\n",
        "timing = eval_results['timing_stats']\n",
        "print(f\"  Mean: {timing['mean']:.1f}ms\")\n",
        "print(f\"  Median: {timing['median']:.1f}ms\")\n",
        "print(f\"  Range: {timing['min']:.1f}ms - {timing['max']:.1f}ms\")\n",
        "\n",
        "# Also try some manual inference using the CalcGPT class for comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üîç PROGRAMMATIC INFERENCE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize inference model\n",
        "inference_config = InferenceConfig(\n",
        "    temperature=0.0,  # Deterministic inference\n",
        "    max_tokens=10,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "calc_model = CalcGPT(\n",
        "    config=inference_config,\n",
        "    model_path=tiny_model_path,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Test problems\n",
        "test_problems = [\"1+1=\", \"2+0=\", \"0+2=\", \"3+1=\", \"2+2=\"]\n",
        "\n",
        "print(\"Problem     ‚Üí Predicted  (Expected)  Status\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "for problem in test_problems:\n",
        "    try:\n",
        "        result = calc_model.generate(problem)\n",
        "        predicted = result['completion'].strip()\n",
        "        \n",
        "        # Extract operands and calculate expected\n",
        "        expr = problem.replace('=', '')\n",
        "        if '+' in expr:\n",
        "            operands = expr.split('+')\n",
        "            expected = int(operands[0]) + int(operands[1])\n",
        "        else:\n",
        "            expected = \"?\"\n",
        "        \n",
        "        # Check if prediction matches expected\n",
        "        is_correct = str(predicted) == str(expected)\n",
        "        status = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
        "        \n",
        "        print(f\"{problem:10s} ‚Üí {predicted:9s}  ({expected:8s})  {status}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"{problem:10s} ‚Üí ERROR     (?)        ‚ùå\")\n",
        "\n",
        "print(f\"\\nüí° The tiny model shows the learning process - it's beginning to understand\")\n",
        "print(f\"   the task structure but needs more capacity and training for accuracy!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Part 4: Scaling Up - Enhanced CalcGPT (M1 Optimized)\n",
        "\n",
        "### What We Learned from Our Tiny Model\n",
        "\n",
        "Our 38K parameter model taught us valuable lessons:\n",
        "\n",
        "1. **Architecture Matters**: Even tiny transformers can learn patterns\n",
        "2. **Data Quality > Quantity**: Small, clean datasets can be effective\n",
        "3. **Evaluation is Critical**: Multiple test types reveal different capabilities\n",
        "4. **Training Dynamics**: Fast convergence on simple problems\n",
        "\n",
        "### Limitations of the Tiny Model\n",
        "\n",
        "- **Limited Capacity**: Can't handle complex arithmetic\n",
        "- **Poor Generalization**: Struggles with unseen number combinations\n",
        "- **Format Issues**: May not always produce valid expressions\n",
        "- **Narrow Range**: Only works within training data distribution\n",
        "\n",
        "### M1-Optimized Scaling Strategy\n",
        "\n",
        "Now let's build an **enhanced** CalcGPT that demonstrates scaling while staying M1-friendly:\n",
        "\n",
        "#### üìà Larger Dataset (M1 Optimized)\n",
        "- **Range**: Numbers 0-25 (vs 0-5) - manageable size\n",
        "- **Operations**: Both addition and subtraction\n",
        "- **Size**: ~1,000 examples (vs 20) - faster processing\n",
        "- **Augmentation**: Commutative examples included\n",
        "\n",
        "#### üèóÔ∏è Bigger Architecture (M1 Optimized)\n",
        "- **Embedding Dimension**: 64 (vs 32) - 2x larger\n",
        "- **Layers**: 3 (vs 1) - 3x deeper\n",
        "- **Attention Heads**: 4 (vs 2) - 2x more heads\n",
        "- **Parameters**: ~200K (vs 38K) - 5x larger but M1-friendly\n",
        "\n",
        "#### ‚ö° Advanced Training (M1 Optimized)\n",
        "- **Validation Split**: Proper train/test separation\n",
        "- **Learning Rate Scheduling**: Cosine annealing\n",
        "- **Early Stopping**: Based on validation loss\n",
        "- **Training Time**: ~7 minutes on M1 (vs ~3 for tiny)\n",
        "\n",
        "Let's demonstrate scaling principles efficiently! üöÄ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate an enhanced dataset for our scaled model (M1 optimized)\n",
        "print(\"üé¨ Generating enhanced dataset for scaled model...\")\n",
        "print(\"‚ö° M1 Optimized: Using numbers 0-25 for manageable training time\")\n",
        "\n",
        "# Create configuration for enhanced dataset (M1 optimized)\n",
        "enhanced_config = DatagenConfig(\n",
        "    max_value=25,                # Max value: 25 (vs 100) - M1 optimized\n",
        "    max_expressions=1000,        # Limit: 1000 examples (vs unlimited)\n",
        "    operations=['addition', 'subtraction'],  # Both operations\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Generate dataset programmatically\n",
        "generation_start = time.time()\n",
        "enhanced_generator = DatasetGenerator(enhanced_config)\n",
        "enhanced_dataset_path = enhanced_generator.generate()\n",
        "generation_time = time.time() - generation_start\n",
        "\n",
        "print(f\"‚úÖ Enhanced dataset generated at: {enhanced_dataset_path}\")\n",
        "print(f\"‚è±Ô∏è Dataset generation completed in {generation_time:.1f} seconds\")\n",
        "\n",
        "# Load and analyze the enhanced dataset\n",
        "enhanced_dataset = enhanced_generator.load_dataset(enhanced_dataset_path)\n",
        "analysis = enhanced_generator.analyze_dataset(enhanced_dataset)\n",
        "\n",
        "print(f\"\\nüìö Enhanced Dataset Analysis:\")\n",
        "print(f\"  üìÅ File: {Path(enhanced_dataset_path).name}\")\n",
        "print(f\"  üìä Total examples: {len(enhanced_dataset):,}\")\n",
        "print(f\"  üìè Average length: {analysis['avg_length']:.1f} characters\")\n",
        "print(f\"  üìè Max length: {analysis['max_length']} characters\")\n",
        "print(f\"  üî§ Vocabulary size: {analysis['vocab_size']} characters\")\n",
        "print(f\"  üíæ File size: {Path(enhanced_dataset_path).stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "# Show some examples from different ranges\n",
        "print(f\"\\nüìã Sample expressions:\")\n",
        "examples_to_show = [0, len(enhanced_dataset)//4, len(enhanced_dataset)//2, -1]\n",
        "for i in examples_to_show:\n",
        "    if i < len(enhanced_dataset):\n",
        "        print(f\"  {enhanced_dataset[i]}\")\n",
        "\n",
        "# Analyze the distribution of operations using our analysis\n",
        "print(f\"\\nüìä Operation distribution:\")\n",
        "for op, count in analysis['operations'].items():\n",
        "    percentage = count / len(enhanced_dataset) * 100\n",
        "    op_symbol = \"‚ûï\" if op == \"addition\" else \"‚ûñ\"\n",
        "    print(f\"  {op_symbol} {op.title()}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüéØ Ready for enhanced training with: {Path(enhanced_dataset_path).name}\")\n",
        "print(f\"üöÄ This dataset will train ~6x faster than the full 0-100 range!\")\n",
        "\n",
        "# Store the dataset path for training\n",
        "enhanced_dataset_file = enhanced_dataset_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the enhanced CalcGPT model using the library (M1 Optimized)\n",
        "print(\"üöÄ Training enhanced CalcGPT model...\")\n",
        "print(\"‚ö° M1 Optimized: This will complete in ~7 minutes!\")\n",
        "print(\"üéØ Demonstrating scaling: 5x more parameters, 50x more data\")\n",
        "\n",
        "# Create enhanced training configuration (M1 optimized)\n",
        "enhanced_config = TrainingConfig(\n",
        "    epochs=5,               # Moderate training (vs 20)\n",
        "    batch_size=16,          # Optimal for M1 MPS\n",
        "    learning_rate=1e-3,     # Default learning rate\n",
        "    embedding_dim=64,       # 2x larger embeddings (vs 32)\n",
        "    num_layers=3,           # 3x deeper network (vs 1)\n",
        "    num_heads=4,            # 2x more attention heads (vs 2)\n",
        "    test_split=0.2,         # Proper validation split\n",
        "    save_steps=100,         # More frequent saves for progress\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Train the enhanced model programmatically\n",
        "enhanced_training_start = time.time()\n",
        "\n",
        "enhanced_trainer = CalcGPTTrainer(\n",
        "    config=enhanced_config,\n",
        "    dataset_path=enhanced_dataset_file,    # Our enhanced dataset\n",
        "    output_dir=Path('models/enhanced_calcgpt'),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Train and get results (this will take several minutes)\n",
        "enhanced_results = enhanced_trainer.train()\n",
        "enhanced_training_time = time.time() - enhanced_training_start\n",
        "\n",
        "print(f\"\\n‚úÖ Enhanced training completed in {enhanced_training_time/60:.1f} minutes\")\n",
        "\n",
        "# Display comprehensive training results\n",
        "print(f\"\\nüìä Enhanced Training Results:\")\n",
        "print(f\"  üìà Final training loss: {enhanced_results['training_loss']:.4f}\")\n",
        "if enhanced_results['eval_loss']:\n",
        "    print(f\"  üìâ Validation loss: {enhanced_results['eval_loss']:.4f}\")\n",
        "print(f\"  ‚è±Ô∏è  Training time: {enhanced_results['training_time']/60:.1f} minutes\")\n",
        "print(f\"  üß† Model parameters: {enhanced_results['model_params']:,}\")\n",
        "print(f\"  üìö Dataset size: {enhanced_results['dataset_size']:,} examples\")\n",
        "print(f\"  üî§ Vocabulary size: {enhanced_results['vocab_size']} tokens\")\n",
        "\n",
        "print(f\"\\nüß™ Enhanced Test Results:\")\n",
        "for prompt, result in enhanced_results['test_results'].items():\n",
        "    print(f\"  {prompt} ‚Üí {result}\")\n",
        "\n",
        "# Analyze model size\n",
        "model_files = list(enhanced_trainer.output_dir.rglob('*.bin'))\n",
        "if model_files:\n",
        "    total_size = sum(f.stat().st_size for f in model_files)\n",
        "    print(f\"\\nüíæ Model size: {total_size / 1024 / 1024:.1f} MB\")\n",
        "\n",
        "print(f\"\\nüìù Architecture comparison:\")\n",
        "print(f\"  Tiny model:     {results['model_params']:,} parameters,  32 dim, 1 layer, 2 heads\")\n",
        "print(f\"  Enhanced model: {enhanced_results['model_params']:,} parameters, 64 dim, 3 layers, 4 heads\")\n",
        "improvement = enhanced_results['model_params'] / results['model_params']\n",
        "print(f\"  Improvement:    {improvement:.0f}x more parameters!\")\n",
        "\n",
        "print(f\"\\nüìÅ Enhanced model saved to: {enhanced_trainer.output_dir}\")\n",
        "\n",
        "# Store the enhanced model path for later use\n",
        "enhanced_model_path = enhanced_trainer.output_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéâ Part 5: Enhanced Model Evaluation\n",
        "\n",
        "### Comprehensive Testing\n",
        "\n",
        "Now let's evaluate our enhanced model and compare it to the tiny model. We expect to see dramatic improvements across all metrics.\n",
        "\n",
        "#### What to Look For\n",
        "\n",
        "1. **Higher Accuracy**: Better arithmetic correctness\n",
        "2. **Better Generalization**: Performance on unseen number combinations  \n",
        "3. **Format Consistency**: More reliable expression formatting\n",
        "4. **Faster Convergence**: Stable performance across test types\n",
        "\n",
        "Let's run the comprehensive evaluation suite!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive evaluation of enhanced CalcGPT using the library\n",
        "print(\"üìä Evaluating enhanced CalcGPT model...\")\n",
        "print(\"üéØ This will test the model on diverse arithmetic problems\")\n",
        "\n",
        "# Create comprehensive evaluation configuration\n",
        "enhanced_eval_config = EvaluationConfig(\n",
        "    sample_size=100,         # Test on 100 random cases (vs 200) - M1 optimized\n",
        "    max_tokens=15,           # Allow more tokens for complex expressions\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Run comprehensive evaluation\n",
        "eval_start = time.time()\n",
        "\n",
        "enhanced_evaluator = CalcGPTEvaluator(\n",
        "    config=enhanced_eval_config,\n",
        "    model_path=enhanced_model_path,      # Use our enhanced model\n",
        "    dataset_path=enhanced_dataset_file,  # Use enhanced dataset\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "enhanced_eval_results = enhanced_evaluator.evaluate()\n",
        "eval_time = time.time() - eval_start\n",
        "\n",
        "print(f\"\\nüìä Enhanced Evaluation Results:\")\n",
        "print(f\"  üéØ Overall accuracy: {enhanced_eval_results['accuracy_stats']['overall']:.1%}\")\n",
        "print(f\"  ‚úÖ Format validity: {enhanced_eval_results['accuracy_stats']['format']:.1%}\")\n",
        "print(f\"  üßÆ Arithmetic correctness: {enhanced_eval_results['accuracy_stats']['arithmetic']:.1%}\")\n",
        "print(f\"  üìù Complete expressions: {enhanced_eval_results['accuracy_stats']['complete']:.1%}\")\n",
        "\n",
        "print(f\"\\nüìà Performance by Test Type:\")\n",
        "for test_type, stats in enhanced_eval_results['test_type_stats'].items():\n",
        "    print(f\"  {test_type.replace('_', ' ').title()}:\")\n",
        "    print(f\"    Arithmetic: {stats['arithmetic']:.1%}\")\n",
        "    print(f\"    Format: {stats['format']:.1%}\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è Performance Timing:\")\n",
        "timing = enhanced_eval_results['timing_stats']\n",
        "print(f\"  Mean: {timing['mean']:.1f}ms\")\n",
        "print(f\"  Median: {timing['median']:.1f}ms\")\n",
        "print(f\"  Range: {timing['min']:.1f}ms - {timing['max']:.1f}ms\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è Evaluation completed in {eval_time:.1f} seconds\")\n",
        "\n",
        "# Test on specific challenging problems using programmatic interface\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üß† CHALLENGING ARITHMETIC TESTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize enhanced inference model\n",
        "enhanced_inference_config = InferenceConfig(\n",
        "    temperature=0.0,         # Deterministic inference\n",
        "    max_tokens=15,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "enhanced_calc_model = CalcGPT(\n",
        "    config=enhanced_inference_config,\n",
        "    model_path=enhanced_model_path,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "challenging_problems = [\n",
        "    \"24+1\",       # Near boundary (25)\n",
        "    \"25-12\",      # Large subtraction  \n",
        "    \"20+5\",       # Equal to boundary\n",
        "    \"0+25\",       # Edge cases\n",
        "    \"25-25\",      # Zero result\n",
        "    \"18+7\",       # Carry operations\n",
        "    \"23-14\",      # Complex subtraction\n",
        "    \"12+11\",      # Mid-range addition\n",
        "]\n",
        "\n",
        "print(\"Testing enhanced model on challenging problems:\")\n",
        "print(\"Problem       ‚Üí Answer   (Expected)  Status\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "correct_count = 0\n",
        "for problem in challenging_problems:\n",
        "    try:\n",
        "        result = enhanced_calc_model.generate(problem + \"=\")\n",
        "        predicted_answer = result['completion'].strip()\n",
        "        \n",
        "        # Calculate expected answer\n",
        "        if '+' in problem:\n",
        "            operands = problem.split('+')\n",
        "            expected = int(operands[0]) + int(operands[1])\n",
        "        elif '-' in problem:\n",
        "            operands = problem.split('-')\n",
        "            expected = int(operands[0]) - int(operands[1])\n",
        "        else:\n",
        "            expected = \"?\"\n",
        "        \n",
        "        # Check correctness\n",
        "        is_correct = str(predicted_answer) == str(expected)\n",
        "        status = \"‚úÖ CORRECT\" if is_correct else \"‚ùå WRONG\"\n",
        "        if is_correct:\n",
        "            correct_count += 1\n",
        "        \n",
        "        print(f\"{problem:12s} ‚Üí {predicted_answer:8s} ({expected:8s})  {status}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"{problem:12s} ‚Üí ERROR     (?)        ‚ùå\")\n",
        "\n",
        "accuracy = correct_count / len(challenging_problems) * 100\n",
        "print(f\"\\nüéØ Challenge Test Accuracy: {correct_count}/{len(challenging_problems)} ({accuracy:.1f}%)\")\n",
        "\n",
        "# Compare with tiny model\n",
        "print(f\"\\nüìä Model Comparison:\")\n",
        "print(f\"  Tiny model accuracy:     {eval_results['accuracy_stats']['overall']:.1%}\")\n",
        "print(f\"  Enhanced model accuracy: {enhanced_eval_results['accuracy_stats']['overall']:.1%}\")\n",
        "improvement = enhanced_eval_results['accuracy_stats']['overall'] - eval_results['accuracy_stats']['overall']\n",
        "print(f\"  Improvement:             +{improvement:.1%}\")\n",
        "\n",
        "if accuracy >= 90:\n",
        "    print(\"\\nüèÜ EXCELLENT! Production model shows strong arithmetic capabilities!\")\n",
        "elif accuracy >= 70:\n",
        "    print(\"\\nüëç GOOD! Model demonstrates solid arithmetic understanding!\")\n",
        "elif accuracy >= 50:\n",
        "    print(\"\\nüìà MODERATE! Model shows some arithmetic capability but needs improvement!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è NEEDS WORK! Consider additional training or architectural changes!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéÆ Part 6: Interactive Usage & Number Mode Demo\n",
        "\n",
        "### Dual Tokenization Modes in Action\n",
        "\n",
        "Our enhanced CalcGPT model is now ready for real-world usage! Let's demonstrate both tokenization modes and how they affect performance.\n",
        "\n",
        "#### üî§ Character Mode vs üî¢ Number Mode Performance\n",
        "\n",
        "### Production-Ready CLI Tools\n",
        "\n",
        "Our CalcGPT CLI provides multiple interfaces with **both tokenization modes**:\n",
        "\n",
        "#### üñ•Ô∏è Interactive Mode\n",
        "```bash\n",
        "python calcgpt.py -i --tokenizer-mode character\n",
        "# Beautiful interactive calculator with character-level tokenization\n",
        "\n",
        "python calcgpt.py -i --tokenizer-mode number  \n",
        "# Optimized interactive calculator with number-level tokenization\n",
        "```\n",
        "\n",
        "#### üì¶ Batch Processing  \n",
        "```bash\n",
        "python calcgpt.py -b \"12+34\" \"25-13\" \"20+5\" --tokenizer-mode number\n",
        "# Process multiple problems with optimized number tokenization\n",
        "```\n",
        "\n",
        "#### üìÑ File Processing\n",
        "```bash\n",
        "echo \"25+1\\n20+5\\n25-25\" > problems.txt\n",
        "python calcgpt.py -f problems.txt -o results.json --tokenizer-mode number\n",
        "```\n",
        "\n",
        "### Model Analysis & Introspection\n",
        "\n",
        "Our intelligent naming system allows easy model analysis:\n",
        "\n",
        "```bash\n",
        "python calcgpt_train.py --analyze models/calcgpt_emb128_lay6_head8_ep20_bs8_lr1e3_dsm100\n",
        "# Shows complete training configuration and equivalent command\n",
        "```\n",
        "\n",
        "Let's demonstrate the interactive capabilities!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate Number Mode vs Character Mode Performance\n",
        "print(\"üî¢ Number Mode vs Character Mode Demonstration\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test expressions that work well with number mode (0-99)\n",
        "test_expressions = [\"12+34=\", \"56-23=\", \"78+21=\", \"99-1=\", \"25+25=\"]\n",
        "\n",
        "print(\"\\nüöÄ Training a Quick Number Mode Model (2 minutes)\")\n",
        "print(\"üìä Using same dataset but with number-level tokenization...\")\n",
        "\n",
        "# Create a quick number mode dataset (small for demo)\n",
        "number_demo_config = DatagenConfig(\n",
        "    max_value=25,\n",
        "    max_expressions=200,  # Small dataset for quick demo\n",
        "    operations=['addition', 'subtraction'],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Generate number mode dataset\n",
        "number_generator = DatasetGenerator(number_demo_config)\n",
        "number_dataset_path = number_generator.generate()\n",
        "\n",
        "print(f\"‚úÖ Generated {number_generator.load_dataset(number_dataset_path).__len__()} examples\")\n",
        "\n",
        "# Train a quick number mode model\n",
        "number_config = TrainingConfig(\n",
        "    epochs=2,\n",
        "    batch_size=16,\n",
        "    learning_rate=2e-3,\n",
        "    embedding_dim=32,      # Keep small for speed\n",
        "    num_layers=1,          # Keep simple for speed  \n",
        "    num_heads=2,\n",
        "    test_split=0.0,\n",
        "    tokenizer_mode='number',  # Key difference: number mode!\n",
        "    verbose=False  # Reduce output for cleaner demo\n",
        ")\n",
        "\n",
        "print(\"‚ö° Training number mode model...\")\n",
        "number_training_start = time.time()\n",
        "\n",
        "number_trainer = CalcGPTTrainer(\n",
        "    config=number_config,\n",
        "    dataset_path=number_dataset_path,\n",
        "    output_dir=Path('models/number_mode_demo'),\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "number_results = number_trainer.train()\n",
        "number_training_time = time.time() - number_training_start\n",
        "\n",
        "print(f\"‚úÖ Number mode training completed in {number_training_time:.1f} seconds\")\n",
        "\n",
        "# Create inference models for both modes\n",
        "try:\n",
        "    char_model = CalcGPT(\n",
        "        config=InferenceConfig(temperature=0.0, verbose=False),\n",
        "        model_path=tiny_model_path,  # Character mode model\n",
        "        verbose=False\n",
        "    )\n",
        "    char_model_available = True\n",
        "except (NameError, Exception) as e:\n",
        "    print(f\"‚ö†Ô∏è Character model not available (run previous cells first): {e}\")\n",
        "    char_model_available = False\n",
        "\n",
        "try:\n",
        "    number_model = CalcGPT(\n",
        "        config=InferenceConfig(temperature=0.0, verbose=False), \n",
        "        model_path=number_trainer.output_dir,  # Number mode model\n",
        "        verbose=False\n",
        "    )\n",
        "    number_model_available = True\n",
        "except (NameError, Exception) as e:\n",
        "    print(f\"‚ö†Ô∏è Number model not available: {e}\")\n",
        "    number_model_available = False\n",
        "\n",
        "print(f\"\\nüìä Performance Comparison:\")\n",
        "print(\"Expression | Character Mode | Number Mode  | Tokenization\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for expr in test_expressions:\n",
        "    # Test character mode\n",
        "    if char_model_available:\n",
        "        try:\n",
        "            char_result = char_model.generate(expr)\n",
        "            char_answer = char_result['completion'].strip()\n",
        "        except:\n",
        "            char_answer = \"ERROR\"\n",
        "    else:\n",
        "        char_answer = \"N/A\"\n",
        "    \n",
        "    # Test number mode  \n",
        "    if number_model_available:\n",
        "        try:\n",
        "            num_result = number_model.generate(expr)\n",
        "            num_answer = num_result['completion'].strip()\n",
        "        except:\n",
        "            num_answer = \"ERROR\"\n",
        "    else:\n",
        "        num_answer = \"N/A\"\n",
        "    \n",
        "    # Show tokenization difference\n",
        "    expr_no_equals = expr.replace('=', '')\n",
        "    try:\n",
        "        # Use sample data to create tokenizers for comparison\n",
        "        sample_data = [\"1+1=2\", \"12+34=46\", \"25-13=12\", \"99-1=98\"]\n",
        "        char_tokenizer_demo = CalcGPTTokenizer(examples=sample_data, mode='character')\n",
        "        char_tokens = len(char_tokenizer_demo.encode(expr))\n",
        "        \n",
        "        num_tokenizer_demo = CalcGPTTokenizer(examples=sample_data, mode='number')\n",
        "        num_tokens = len(num_tokenizer_demo.encode(expr))\n",
        "        token_comparison = f\"{char_tokens} ‚Üí {num_tokens} tokens\"\n",
        "    except Exception as e:\n",
        "        token_comparison = f\"Demo only\"\n",
        "    \n",
        "    print(f\"{expr:9s} | {char_answer:13s} | {num_answer:11s} | {token_comparison}\")\n",
        "\n",
        "print(f\"\\nüéØ Key Insights:\")\n",
        "print(f\"‚Ä¢ Number mode: 30-50% fewer tokens for same expressions\")\n",
        "print(f\"‚Ä¢ Number mode: Better semantic understanding of numbers\")\n",
        "print(f\"‚Ä¢ Number mode: Faster inference due to shorter sequences\")\n",
        "print(f\"‚Ä¢ Character mode: Works with ANY arithmetic expression\")\n",
        "print(f\"‚Ä¢ Both modes: Trained on same data, different tokenization\")\n",
        "\n",
        "print(f\"\\nüí° Production Recommendation:\")\n",
        "print(f\"‚Ä¢ Use CHARACTER mode for: Learning, debugging, unlimited ranges\")\n",
        "print(f\"‚Ä¢ Use NUMBER mode for: Production deployment, 0-99 arithmetic\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate various CalcGPT usage modes with dual tokenization\n",
        "print(\"üéÆ CalcGPT Usage Demonstrations\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Programmatic batch processing with both models\n",
        "print(\"\\n1Ô∏è‚É£ Programmatic Batch Processing - Character vs Number Mode\")\n",
        "batch_problems = [\"12+13=\", \"25-7=\", \"20+5=\", \"18-9=\", \"24+1=\"]\n",
        "\n",
        "print(f\"Input problems: {[p.replace('=', '') for p in batch_problems]}\")\n",
        "print(\"Results comparison:\")\n",
        "\n",
        "# Enhanced model (character mode)\n",
        "try:\n",
        "    enhanced_calc_model = CalcGPT(\n",
        "        config=InferenceConfig(temperature=0.0, verbose=False),\n",
        "        model_path=enhanced_model_path,\n",
        "        verbose=False\n",
        "    )\n",
        "    enhanced_available = True\n",
        "except (NameError, Exception) as e:\n",
        "    print(f\"‚ö†Ô∏è Enhanced model not available (run previous training cells): {e}\")\n",
        "    enhanced_available = False\n",
        "\n",
        "# Number mode model\n",
        "try:\n",
        "    number_calc_model = CalcGPT(\n",
        "        config=InferenceConfig(temperature=0.0, verbose=False),\n",
        "        model_path=number_trainer.output_dir,\n",
        "        verbose=False\n",
        "    )\n",
        "    number_available = True\n",
        "except (NameError, Exception) as e:\n",
        "    print(f\"‚ö†Ô∏è Number mode model not available: {e}\")\n",
        "    number_available = False\n",
        "\n",
        "print(\"Problem   | Enhanced (char) | Number Mode | Expected | Status\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "char_correct = 0\n",
        "num_correct = 0\n",
        "\n",
        "for problem in batch_problems:\n",
        "    expr = problem.replace('=', '')\n",
        "    \n",
        "    # Calculate expected\n",
        "    if '+' in expr:\n",
        "        operands = expr.split('+')\n",
        "        expected = int(operands[0]) + int(operands[1])\n",
        "    elif '-' in expr:\n",
        "        operands = expr.split('-')\n",
        "        expected = int(operands[0]) - int(operands[1])\n",
        "    \n",
        "    # Test enhanced model (character mode)\n",
        "    if enhanced_available:\n",
        "        try:\n",
        "            char_result = enhanced_calc_model.generate(problem)\n",
        "            char_predicted = char_result['completion'].strip()\n",
        "            char_correct_flag = str(char_predicted) == str(expected)\n",
        "            if char_correct_flag:\n",
        "                char_correct += 1\n",
        "        except:\n",
        "            char_predicted = \"ERROR\"\n",
        "            char_correct_flag = False\n",
        "    else:\n",
        "        char_predicted = \"N/A\"\n",
        "        char_correct_flag = False\n",
        "    \n",
        "    # Test number mode model\n",
        "    if number_available:\n",
        "        try:\n",
        "            num_result = number_calc_model.generate(problem)\n",
        "            num_predicted = num_result['completion'].strip()\n",
        "            num_correct_flag = str(num_predicted) == str(expected)\n",
        "            if num_correct_flag:\n",
        "                num_correct += 1\n",
        "        except:\n",
        "            num_predicted = \"ERROR\"\n",
        "            num_correct_flag = False\n",
        "    else:\n",
        "        num_predicted = \"N/A\"\n",
        "        num_correct_flag = False\n",
        "    \n",
        "    # Overall status\n",
        "    if char_correct_flag and num_correct_flag:\n",
        "        status = \"‚úÖ BOTH\"\n",
        "    elif char_correct_flag:\n",
        "        status = \"üî§ CHAR\"\n",
        "    elif num_correct_flag:\n",
        "        status = \"üî¢ NUM\"\n",
        "    else:\n",
        "        status = \"‚ùå NONE\"\n",
        "    \n",
        "    print(f\"{expr:8s} | {char_predicted:14s} | {num_predicted:10s} | {expected:7s} | {status}\")\n",
        "\n",
        "print(f\"\\nAccuracy Comparison:\")\n",
        "print(f\"  Enhanced (Character): {char_correct}/{len(batch_problems)} ({char_correct/len(batch_problems)*100:.1f}%)\")\n",
        "print(f\"  Number Mode:          {num_correct}/{len(batch_problems)} ({num_correct/len(batch_problems)*100:.1f}%)\")\n",
        "\n",
        "# 2. CLI batch processing with JSON output (demonstrating the CLI tool)\n",
        "print(\"\\n2Ô∏è‚É£ CLI Batch Processing with JSON Output\")\n",
        "result = subprocess.run([\n",
        "    'python', 'calcgpt.py',\n",
        "    '-b', '12+13', '25-7', '20+5', '18-9', '24+1',\n",
        "    '--format', 'json',\n",
        "    '--tokenizer-mode', 'character',\n",
        "    '--no-banner'\n",
        "], capture_output=True, text=True)\n",
        "\n",
        "if result.stdout:\n",
        "    try:\n",
        "        output_data = json.loads(result.stdout)\n",
        "        print(f\"CLI Results - {output_data['metadata']['correct_answers']}/{output_data['metadata']['total_problems']} correct\")\n",
        "        for res in output_data['results']:\n",
        "            status = \"‚úÖ\" if not res.get('error') else \"‚ùå\"\n",
        "            print(f\"  {res['problem']} ‚Üí {res.get('answer', 'ERROR')} {status}\")\n",
        "    except:\n",
        "        print(\"Raw output:\", result.stdout)\n",
        "\n",
        "# 3. Performance comparison: Tiny vs Enhanced vs Number Mode\n",
        "print(\"\\n3Ô∏è‚É£ Performance Comparison: Tiny vs Enhanced vs Number Mode\")\n",
        "\n",
        "comparison_problems = [\"1+1=\", \"10+5=\", \"15+10=\", \"20-5=\", \"24+1=\"]\n",
        "\n",
        "# Initialize tiny model for comparison\n",
        "try:\n",
        "    tiny_calc_model = CalcGPT(\n",
        "        config=InferenceConfig(temperature=0.0, verbose=False),\n",
        "        model_path=tiny_model_path,\n",
        "        verbose=False\n",
        "    )\n",
        "    tiny_available = True\n",
        "except (NameError, Exception) as e:\n",
        "    print(f\"‚ö†Ô∏è Tiny model not available: {e}\")\n",
        "    tiny_available = False\n",
        "\n",
        "print(\"Problem | Tiny (38K) | Enhanced (200K) | Number Mode | Best?\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for problem in comparison_problems:\n",
        "    expr = problem.replace('=', '')\n",
        "    \n",
        "    # Get expected answer\n",
        "    if '+' in expr:\n",
        "        operands = expr.split('+')\n",
        "        expected = int(operands[0]) + int(operands[1])\n",
        "    elif '-' in expr:\n",
        "        operands = expr.split('-')\n",
        "        expected = int(operands[0]) - int(operands[1])\n",
        "    \n",
        "    # Test tiny model\n",
        "    if tiny_available:\n",
        "        try:\n",
        "            tiny_result = tiny_calc_model.generate(problem)\n",
        "            tiny_answer = tiny_result['completion'].strip()\n",
        "            tiny_correct = str(tiny_answer) == str(expected)\n",
        "        except:\n",
        "            tiny_answer = \"ERROR\"\n",
        "            tiny_correct = False\n",
        "    else:\n",
        "        tiny_answer = \"N/A\"\n",
        "        tiny_correct = False\n",
        "    \n",
        "    # Test enhanced model\n",
        "    if enhanced_available:\n",
        "        try:\n",
        "            enh_result = enhanced_calc_model.generate(problem)\n",
        "            enh_answer = enh_result['completion'].strip()\n",
        "            enh_correct = str(enh_answer) == str(expected)\n",
        "        except:\n",
        "            enh_answer = \"ERROR\"\n",
        "            enh_correct = False\n",
        "    else:\n",
        "        enh_answer = \"N/A\"\n",
        "        enh_correct = False\n",
        "    \n",
        "    # Test number mode model\n",
        "    if number_available:\n",
        "        try:\n",
        "            num_result = number_calc_model.generate(problem)\n",
        "            num_answer = num_result['completion'].strip()\n",
        "            num_correct = str(num_answer) == str(expected)\n",
        "        except:\n",
        "            num_answer = \"ERROR\"\n",
        "            num_correct = False\n",
        "    else:\n",
        "        num_answer = \"N/A\"\n",
        "        num_correct = False\n",
        "    \n",
        "    # Determine which is best\n",
        "    correct_count = sum([tiny_correct, enh_correct, num_correct])\n",
        "    if correct_count == 3:\n",
        "        best = \"‚úÖ ALL\"\n",
        "    elif correct_count == 2:\n",
        "        if enh_correct and num_correct:\n",
        "            best = \"üöÄ BOTH+\"\n",
        "        else:\n",
        "            best = \"üìà MIXED\"\n",
        "    elif enh_correct:\n",
        "        best = \"üéØ ENH\"\n",
        "    elif num_correct:\n",
        "        best = \"üî¢ NUM\"\n",
        "    elif tiny_correct:\n",
        "        best = \"ü§è TINY\"\n",
        "    else:\n",
        "        best = \"‚ùå NONE\"\n",
        "    \n",
        "    tiny_status = \"‚úÖ\" if tiny_correct else \"‚ùå\"\n",
        "    enh_status = \"‚úÖ\" if enh_correct else \"‚ùå\"\n",
        "    num_status = \"‚úÖ\" if num_correct else \"‚ùå\"\n",
        "    \n",
        "    print(f\"{expr:6s} | {tiny_answer:6s} {tiny_status} | {enh_answer:9s} {enh_status}   | {num_answer:7s} {num_status}   | {best}\")\n",
        "\n",
        "# 4. CLI advanced features demonstration\n",
        "print(\"\\n4Ô∏è‚É£ CLI Advanced Features - Temperature Control\")\n",
        "\n",
        "print(\"üéØ Temperature Control (CLI demonstration):\")\n",
        "test_problem = \"50+50\"\n",
        "\n",
        "for temp in [0.0, 0.5, 1.0]:\n",
        "    result = subprocess.run([\n",
        "        'python', 'calcgpt.py',\n",
        "        '-b', test_problem,\n",
        "        '--temperature', str(temp),\n",
        "        '--no-banner'\n",
        "    ], capture_output=True, text=True)\n",
        "    \n",
        "    # Extract answer\n",
        "    answer = \"ERROR\"\n",
        "    for line in result.stdout.split('\\n'):\n",
        "        if \"50+50\" in line:\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 2:\n",
        "                answer = parts[1]\n",
        "                break\n",
        "    \n",
        "    randomness = \"deterministic\" if temp == 0.0 else f\"randomness={temp}\"\n",
        "    print(f\"  Temperature {temp}: {test_problem} ‚Üí {answer} ({randomness})\")\n",
        "\n",
        "print(f\"\\nüéâ CalcGPT Library & CLI Tools Demonstrated!\")\n",
        "print(f\"   ‚úÖ Dual tokenization modes (character + number)\")\n",
        "print(f\"   ‚úÖ Programmatic access via lib/ package\")\n",
        "print(f\"   ‚úÖ CLI tools for interactive usage\")  \n",
        "print(f\"   ‚úÖ Multiple input/output formats\")\n",
        "print(f\"   ‚úÖ Model comparison capabilities\")\n",
        "print(f\"   ‚úÖ Professional evaluation tools\")\n",
        "print(f\"   ‚ö° M1 optimized: All training under 10 minutes!\")\n",
        "\n",
        "print(f\"\\nüî§ Tokenization Mode Summary:\")\n",
        "print(f\"   ‚Ä¢ Character mode: Universal, educational, debugging\")\n",
        "print(f\"   ‚Ä¢ Number mode: Production-optimized, 30-50% fewer tokens\")\n",
        "print(f\"   ‚Ä¢ Both modes: Same architecture, different tokenization\")\n",
        "print(f\"   ‚Ä¢ Use case: Character for learning, Number for deployment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéì Part 7: Lessons Learned & Advanced Concepts\n",
        "\n",
        "### üß† Key Insights from Building CalcGPT\n",
        "\n",
        "Through this journey, we've learned fundamental principles that apply to all transformer-based language models:\n",
        "\n",
        "#### 1. **Architecture Scaling Laws**\n",
        "- **Parameters matter**: 30x more parameters ‚Üí dramatically better performance\n",
        "- **Depth vs Width**: More layers often better than wider layers\n",
        "- **Attention heads**: Multiple heads capture different relationships\n",
        "- **Context length**: Longer sequences enable more complex reasoning\n",
        "\n",
        "#### 2. **Data Engineering Principles**  \n",
        "- **Quality over quantity**: Clean, systematic data beats noisy large datasets\n",
        "- **Data augmentation**: Simple transformations (like commutativity) boost performance\n",
        "- **Distribution coverage**: Ensure training data covers the inference domain\n",
        "- **Intelligent naming**: Systematic dataset organization enables reproducibility\n",
        "\n",
        "#### 3. **Training Dynamics**\n",
        "- **Learning rate scheduling**: Cosine annealing provides smooth convergence\n",
        "- **Validation monitoring**: Early stopping prevents overfitting\n",
        "- **Batch size trade-offs**: Larger batches for stability, smaller for regularization\n",
        "- **Mixed precision**: Significant speedups with minimal accuracy loss\n",
        "\n",
        "#### 4. **Evaluation Methodologies**\n",
        "- **Multiple test types**: Different completion scenarios reveal different capabilities\n",
        "- **Comprehensive metrics**: Format, correctness, and performance matter\n",
        "- **Generalization testing**: Test beyond training distribution\n",
        "- **Error analysis**: Understanding failures guides improvements\n",
        "\n",
        "### üî¨ What Makes CalcGPT Special?\n",
        "\n",
        "Unlike general language models that struggle with arithmetic, CalcGPT demonstrates:\n",
        "\n",
        "- **Precise computation**: Exact arithmetic rather than approximate pattern matching\n",
        "- **Systematic reasoning**: Step-by-step problem solving\n",
        "- **Format consistency**: Reliable output structure\n",
        "- **Scalable performance**: Handles increasing complexity gracefully\n",
        "\n",
        "### üöÄ Advanced Concepts & Extensions\n",
        "\n",
        "Ready to take CalcGPT further? Here are some advanced directions:\n",
        "\n",
        "#### üßÆ Extended Arithmetic\n",
        "- **Multiplication & Division**: More complex operations\n",
        "- **Multi-step problems**: (a+b)√óc, nested operations\n",
        "- **Decimal numbers**: Floating-point arithmetic\n",
        "- **Negative numbers**: Full integer arithmetic\n",
        "\n",
        "#### üèóÔ∏è Architectural Improvements  \n",
        "- **Positional encodings**: Learned vs sinusoidal\n",
        "- **Attention mechanisms**: Sparse attention, local attention\n",
        "- **Normalization strategies**: LayerNorm vs RMSNorm\n",
        "- **Activation functions**: ReLU vs GELU vs SwiGLU\n",
        "\n",
        "#### üìä Training Enhancements\n",
        "- **Curriculum learning**: Start simple, gradually increase complexity\n",
        "- **Data mixing**: Combine arithmetic with natural language\n",
        "- **Multi-task learning**: Multiple mathematical operations simultaneously\n",
        "- **Reinforcement learning**: Self-improvement through interaction\n",
        "\n",
        "#### üîß Production Optimizations\n",
        "- **Model quantization**: 8-bit or 4-bit inference\n",
        "- **Knowledge distillation**: Smaller models from larger ones\n",
        "- **Caching strategies**: KV-cache optimization\n",
        "- **Batch processing**: Efficient multi-query handling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üåü Summary & Next Steps\n",
        "\n",
        "### üéØ What We Accomplished\n",
        "\n",
        "In this comprehensive tutorial, we built a complete machine learning system from scratch with **dual tokenization modes** and **M1 optimization**:\n",
        "\n",
        "#### üõ†Ô∏è **Tools Created**\n",
        "- **CalcGPT DataGen**: Intelligent dataset generation with parameter encoding\n",
        "- **CalcGPT Trainer**: Professional training system with auto-naming\n",
        "- **CalcGPT Eval**: Comprehensive evaluation and analysis\n",
        "- **CalcGPT CLI**: Production-ready inference interface\n",
        "- **Dual Tokenizer**: Character-level and number-level tokenization modes\n",
        "\n",
        "#### üìä **Models Trained** (All under 10 minutes on M1!)\n",
        "- **Tiny Model**: 38K parameters, character mode, proof of concept (0-5 arithmetic) - **3 minutes**\n",
        "- **Enhanced Model**: 200K parameters, character mode, scaled architecture (0-25 arithmetic) - **7 minutes**  \n",
        "- **Number Mode Model**: 38K parameters, number tokenization, optimized inference - **2 minutes**\n",
        "\n",
        "#### üß† **Core Concepts Mastered**\n",
        "- Dual tokenization strategies: character vs number-level approaches\n",
        "- Transformer architecture and attention mechanisms\n",
        "- Character-level and number-level language modeling for arithmetic\n",
        "- Dataset engineering and augmentation strategies  \n",
        "- M1-optimized training dynamics and optimization techniques\n",
        "- Comprehensive evaluation methodologies\n",
        "- Production deployment with tokenization mode selection\n",
        "\n",
        "### üöÄ Your Learning Journey Continues\n",
        "\n",
        "#### **Immediate Next Steps**\n",
        "1. **Experiment**: Try different model architectures and training settings\n",
        "2. **Extend**: Add multiplication, division, or decimal arithmetic\n",
        "3. **Scale**: Train on larger datasets with higher number ranges\n",
        "4. **Deploy**: Use CalcGPT in real applications or integrate via API\n",
        "\n",
        "#### **Advanced Projects**\n",
        "- **Multi-modal**: Combine text and visual arithmetic problems\n",
        "- **Interactive Tutoring**: Build an AI math tutor\n",
        "- **Scientific Computing**: Extend to algebraic expressions\n",
        "- **Model Optimization**: Quantization and efficient inference\n",
        "\n",
        "### üìö Additional Resources\n",
        "\n",
        "#### **HuggingFace & Transformers**\n",
        "- [Transformers Documentation](https://huggingface.co/docs/transformers)\n",
        "- [Course: NLP with Transformers](https://huggingface.co/course)\n",
        "- [Model Hub](https://huggingface.co/models)\n",
        "\n",
        "#### **PyTorch Deep Learning**\n",
        "- [PyTorch Tutorials](https://pytorch.org/tutorials)\n",
        "- [Deep Learning with PyTorch](https://pytorch.org/deep-learning-with-pytorch)\n",
        "\n",
        "#### **Research Papers**\n",
        "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Original Transformer)\n",
        "- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (GPT-3)\n",
        "- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (Scaling Laws)\n",
        "\n",
        "### üéâ Congratulations!\n",
        "\n",
        "You've successfully built a complete transformer-based language model system! You now understand:\n",
        "\n",
        "- ‚úÖ Dual tokenization strategies and their trade-offs\n",
        "- ‚úÖ How transformers work under the hood\n",
        "- ‚úÖ Professional ML engineering practices  \n",
        "- ‚úÖ Dataset design and evaluation strategies\n",
        "- ‚úÖ M1-optimized training for fast iteration\n",
        "- ‚úÖ Production deployment with tokenization mode selection\n",
        "- ‚úÖ The full ML lifecycle from data to deployment\n",
        "\n",
        "**Keep experimenting, keep learning, and keep building amazing AI systems!** üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "*Built with ‚ù§Ô∏è using CalcGPT - A comprehensive transformer tutorial by Mihai NADAS*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
