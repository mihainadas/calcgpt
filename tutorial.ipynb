{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Tutorial: Learning Arithmetic with PyTorch\n",
    "\n",
    "## Project Overview and Objectives\n",
    "\n",
    "### What are we building?\n",
    "In this comprehensive tutorial, we'll implement a **Transformer model from scratch** using PyTorch to solve a fascinating problem: teaching a neural network to perform basic arithmetic operations. Our model will learn to understand and generate mathematical expressions involving addition and subtraction.\n",
    "\n",
    "### Learning Goals\n",
    "By the end of this tutorial, you will have:\n",
    "- **Built a complete Transformer architecture** with multi-head attention, positional encoding, and feed-forward layers\n",
    "- **Understood sequence-to-sequence learning** in the context of mathematical reasoning\n",
    "- **Implemented a custom tokenizer** for mathematical expressions\n",
    "- **Trained and evaluated** a neural network on symbolic reasoning tasks\n",
    "- **Gained hands-on experience** with attention mechanisms and their role in learning structured patterns\n",
    "\n",
    "### The Dataset Challenge\n",
    "We'll use `data/calc.txt` which contains 145 simple arithmetic equations like:\n",
    "- `3+4=7` (addition problems)\n",
    "- `8-5=3` (subtraction problems)\n",
    "- Single-digit operands with results up to 18\n",
    "\n",
    "This seemingly simple task is actually quite challenging for neural networks because it requires:\n",
    "1. **Understanding symbolic relationships** between numbers and operators\n",
    "2. **Learning mathematical rules** rather than just pattern matching\n",
    "3. **Generalizing arithmetic operations** across different number combinations\n",
    "4. **Sequential reasoning** to process left-to-right mathematical expressions\n",
    "\n",
    "### Why Transformers for Arithmetic?\n",
    "Transformers excel at this task because:\n",
    "- **Attention mechanisms** can learn to focus on relevant parts of the equation\n",
    "- **Self-attention** helps the model understand relationships between operands and operators  \n",
    "- **Positional encoding** maintains the order of mathematical operations\n",
    "- **Parallel processing** allows efficient training on sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Data Loading and Exploration\n",
    "\n",
    "### Understanding Our Dataset\n",
    "Before we can train our transformer, we need to understand exactly what we're working with. Our dataset consists of simple arithmetic expressions in the format `operand1 operator operand2 = result`. \n",
    "\n",
    "### Why Data Loading Matters\n",
    "Proper data loading is crucial because:\n",
    "- **Quality assessment**: We need to verify our data is clean and consistent\n",
    "- **Pattern recognition**: Understanding the data structure helps us design appropriate tokenization\n",
    "- **Training efficiency**: Knowing dataset size helps us plan batch sizes and epochs\n",
    "- **Debugging**: If our model fails, we first check if the data is correctly loaded\n",
    "\n",
    "Let's load our arithmetic dataset and examine its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file 'data/calc.txt'...\n",
      "loaded 145 lines from 'data/calc.txt'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "data_file = \"data/calc.txt\"\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"Load arithmetic expressions from file and clean them.\"\"\"\n",
    "    with open(file_name, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Clean the data by removing whitespace and empty lines\n",
    "    cleaned_data = [line.strip() for line in lines if line.strip()]\n",
    "    return cleaned_data\n",
    "\n",
    "print(\"Loading file '{}'...\".format(data_file))\n",
    "data = load_data(data_file)\n",
    "print(\"Loaded {} lines from '{}'\".format(len(data), data_file))\n",
    "\n",
    "# Let's explore our data\n",
    "print(\"\\n=== Data Exploration ===\")\n",
    "print(\"First 10 examples:\")\n",
    "for i, example in enumerate(data[:10]):\n",
    "    print(f\"  {i+1:2d}: {example}\")\n",
    "\n",
    "print(f\"\\nLast 5 examples:\")\n",
    "for i, example in enumerate(data[-5:], len(data)-4):\n",
    "    print(f\"  {i:2d}: {example}\")\n",
    "\n",
    "# Analyze the structure\n",
    "print(f\"\\n=== Data Analysis ===\")\n",
    "operations = []\n",
    "operands = []\n",
    "results = []\n",
    "\n",
    "for expr in data:\n",
    "    # Parse each expression\n",
    "    if '+' in expr:\n",
    "        left, right_and_result = expr.split('+')\n",
    "        operator = '+'\n",
    "        right, result = right_and_result.split('=')\n",
    "    elif '-' in expr:\n",
    "        left, right_and_result = expr.split('-')\n",
    "        operator = '-'\n",
    "        right, result = right_and_result.split('=')\n",
    "    \n",
    "    operations.append(operator)\n",
    "    operands.extend([int(left), int(right)])\n",
    "    results.append(int(result))\n",
    "\n",
    "print(f\"Operations distribution: {Counter(operations)}\")\n",
    "print(f\"Operand range: {min(operands)} to {max(operands)}\")\n",
    "print(f\"Result range: {min(results)} to {max(results)}\")\n",
    "print(f\"Unique operands: {sorted(set(operands))}\")\n",
    "print(f\"Unique results: {sorted(set(results))}\")\n",
    "\n",
    "# Calculate expression lengths\n",
    "expr_lengths = [len(expr) for expr in data]\n",
    "print(f\"Expression length range: {min(expr_lengths)} to {max(expr_lengths)} characters\")\n",
    "print(f\"Average expression length: {np.mean(expr_lengths):.1f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Vocabulary and Tokenization\n",
    "\n",
    "### The Foundation of Language Models\n",
    "Before our transformer can process mathematical expressions, we need to convert text into numbers that the neural network can understand. This process involves two key steps:\n",
    "1. **Tokenization**: Breaking expressions into individual tokens (characters/symbols)\n",
    "2. **Vocabulary mapping**: Creating bidirectional mappings between tokens and integer IDs\n",
    "\n",
    "### Why Character-Level Tokenization?\n",
    "For our arithmetic task, we'll use character-level tokenization because:\n",
    "- **Simplicity**: Each character (0-9, +, -, =) is a meaningful unit\n",
    "- **Completeness**: We can represent any arithmetic expression with a small vocabulary\n",
    "- **Generalization**: The model learns fundamental relationships between symbols\n",
    "- **No OOV issues**: Out-of-vocabulary problems are eliminated with character-level tokens\n",
    "\n",
    "### Key Components We'll Create:\n",
    "1. **Token-to-ID mapping**: Convert characters to integers for neural network input\n",
    "2. **ID-to-token mapping**: Convert model outputs back to readable characters  \n",
    "3. **Special tokens**: Add padding, start-of-sequence, and end-of-sequence markers\n",
    "4. **Encoding/decoding functions**: Transform between text and tensor representations\n",
    "\n",
    "Let's build our vocabulary system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArithmeticTokenizer:\n",
    "    \"\"\"A character-level tokenizer for arithmetic expressions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Extract all unique characters from our data\n",
    "        all_chars = set()\n",
    "        for expr in data:\n",
    "            all_chars.update(expr)\n",
    "        \n",
    "        # Define special tokens\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.SOS_TOKEN = '<SOS>'  # Start of sequence\n",
    "        self.EOS_TOKEN = '<EOS>'  # End of sequence\n",
    "        \n",
    "        # Create vocabulary: special tokens + regular characters\n",
    "        self.vocab = [self.PAD_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN] + sorted(all_chars)\n",
    "        \n",
    "        # Create mappings\n",
    "        self.char_to_id = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        self.id_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
    "        \n",
    "        # Store important indices\n",
    "        self.pad_id = self.char_to_id[self.PAD_TOKEN]\n",
    "        self.sos_id = self.char_to_id[self.SOS_TOKEN]\n",
    "        self.eos_id = self.char_to_id[self.EOS_TOKEN]\n",
    "        \n",
    "        print(f\"Vocabulary created with {len(self.vocab)} tokens:\")\n",
    "        print(f\"Tokens: {self.vocab}\")\n",
    "        print(f\"Special token IDs: PAD={self.pad_id}, SOS={self.sos_id}, EOS={self.eos_id}\")\n",
    "    \n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        \"\"\"Convert text to list of token IDs.\"\"\"\n",
    "        if add_special_tokens:\n",
    "            ids = [self.sos_id]\n",
    "            ids.extend([self.char_to_id[char] for char in text])\n",
    "            ids.append(self.eos_id)\n",
    "        else:\n",
    "            ids = [self.char_to_id[char] for char in text]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids, remove_special_tokens=True):\n",
    "        \"\"\"Convert list of token IDs back to text.\"\"\"\n",
    "        chars = [self.id_to_char[id] for id in ids]\n",
    "        if remove_special_tokens:\n",
    "            # Remove special tokens\n",
    "            chars = [char for char in chars if char not in [self.PAD_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN]]\n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "# Create our tokenizer\n",
    "tokenizer = ArithmeticTokenizer()\n",
    "\n",
    "# Test the tokenizer\n",
    "print(f\"\\n=== Tokenizer Testing ===\")\n",
    "test_expr = \"3+4=7\"\n",
    "print(f\"Original: '{test_expr}'\")\n",
    "\n",
    "encoded = tokenizer.encode(test_expr)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Tokens: {[tokenizer.id_to_char[id] for id in encoded]}\")\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Decoded: '{decoded}'\")\n",
    "\n",
    "# Test with a few more examples\n",
    "print(f\"\\n=== More Examples ===\")\n",
    "for expr in data[:5]:\n",
    "    encoded = tokenizer.encode(expr)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(f\"'{expr}' → {encoded} → '{decoded}'\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Dataset Creation and Preprocessing\n",
    "\n",
    "### Preparing Data for Training\n",
    "Now that we have our tokenizer, we need to create a PyTorch Dataset that can:\n",
    "1. **Convert expressions to tensors**: Transform text into numerical inputs the model can process\n",
    "2. **Handle variable lengths**: Pad sequences to create uniform batch sizes\n",
    "3. **Create input-output pairs**: For sequence-to-sequence learning, we need both input and target sequences\n",
    "4. **Enable efficient batching**: Organize data for parallel processing\n",
    "\n",
    "### Sequence-to-Sequence Setup\n",
    "For our arithmetic task, we'll use a **teacher forcing** approach during training:\n",
    "- **Input sequence**: The entire expression including the equals sign (`3+4=`)\n",
    "- **Target sequence**: The expression shifted by one position (`+4=7`)\n",
    "- **Prediction task**: Given `3+4=`, predict the next character at each position\n",
    "\n",
    "This setup allows the model to learn the relationship between mathematical operations and their results.\n",
    "\n",
    "### Key Preprocessing Steps:\n",
    "1. **Tokenization**: Convert text to token IDs\n",
    "2. **Padding**: Ensure all sequences have the same length\n",
    "3. **Tensor conversion**: Create PyTorch tensors for efficient computation\n",
    "4. **Train/validation split**: Separate data for training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArithmeticDataset(Dataset):\n",
    "    \"\"\"Dataset for arithmetic expressions.\"\"\"\n",
    "    \n",
    "    def __init__(self, expressions, tokenizer, max_length=None):\n",
    "        self.expressions = expressions\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Calculate max length if not provided\n",
    "        if max_length is None:\n",
    "            encoded_lengths = [len(tokenizer.encode(expr)) for expr in expressions]\n",
    "            self.max_length = max(encoded_lengths)\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            \n",
    "        print(f\"Dataset created with {len(expressions)} expressions\")\n",
    "        print(f\"Maximum sequence length: {self.max_length}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.expressions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        expr = self.expressions[idx]\n",
    "        \n",
    "        # Encode the expression\n",
    "        encoded = self.tokenizer.encode(expr)\n",
    "        \n",
    "        # Create input and target sequences\n",
    "        # Input: full sequence, Target: sequence shifted by 1 position\n",
    "        input_seq = encoded[:-1]  # Remove last token for input\n",
    "        target_seq = encoded[1:]  # Remove first token for target\n",
    "        \n",
    "        # Pad sequences to max_length\n",
    "        input_seq = self.pad_sequence(input_seq, self.max_length - 1)\n",
    "        target_seq = self.pad_sequence(target_seq, self.max_length - 1)\n",
    "        \n",
    "        return {\n",
    "            'input': torch.tensor(input_seq, dtype=torch.long),\n",
    "            'target': torch.tensor(target_seq, dtype=torch.long),\n",
    "            'original': expr\n",
    "        }\n",
    "    \n",
    "    def pad_sequence(self, seq, max_len):\n",
    "        \"\"\"Pad sequence to max_len with PAD tokens.\"\"\"\n",
    "        if len(seq) >= max_len:\n",
    "            return seq[:max_len]\n",
    "        else:\n",
    "            return seq + [self.tokenizer.pad_id] * (max_len - len(seq))\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size = len(data) - train_size\n",
    "\n",
    "# Shuffle data before splitting\n",
    "shuffled_data = data.copy()\n",
    "random.shuffle(shuffled_data)\n",
    "\n",
    "train_data = shuffled_data[:train_size]\n",
    "val_data = shuffled_data[train_size:]\n",
    "\n",
    "print(f\"Data split: {train_size} training, {val_size} validation examples\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ArithmeticDataset(train_data, tokenizer)\n",
    "val_dataset = ArithmeticDataset(val_data, tokenizer, max_length=train_dataset.max_length)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Created data loaders with batch size {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Test the dataset\n",
    "print(f\"\\n=== Dataset Testing ===\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch input shape: {sample_batch['input'].shape}\")\n",
    "print(f\"Batch target shape: {sample_batch['target'].shape}\")\n",
    "\n",
    "# Show a few examples\n",
    "for i in range(3):\n",
    "    input_seq = sample_batch['input'][i]\n",
    "    target_seq = sample_batch['target'][i]\n",
    "    original = sample_batch['original'][i]\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Original: '{original}'\")\n",
    "    print(f\"  Input:    {input_seq.tolist()}\")\n",
    "    print(f\"  Target:   {target_seq.tolist()}\")\n",
    "    print(f\"  Input tokens:  {[tokenizer.id_to_char[id.item()] for id in input_seq]}\")\n",
    "    print(f\"  Target tokens: {[tokenizer.id_to_char[id.item()] for id in target_seq]}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Transformer Architecture Implementation\n",
    "\n",
    "### Understanding the Transformer\n",
    "The Transformer architecture, introduced in \"Attention Is All You Need\" (Vaswani et al., 2017), revolutionized sequence modeling by relying entirely on attention mechanisms instead of recurrence or convolution.\n",
    "\n",
    "### Key Components We'll Implement:\n",
    "\n",
    "#### 1. **Multi-Head Self-Attention**\n",
    "- **Purpose**: Allows the model to focus on different parts of the input sequence simultaneously\n",
    "- **Mechanism**: Computes attention weights to determine which tokens are most relevant for each position\n",
    "- **Multiple heads**: Each head can learn different types of relationships (syntactic, semantic, positional)\n",
    "\n",
    "#### 2. **Positional Encoding**\n",
    "- **Problem**: Transformers have no inherent notion of sequence order\n",
    "- **Solution**: Add position information to token embeddings using sinusoidal functions\n",
    "- **Benefit**: Allows the model to understand that `3+4` is different from `4+3`\n",
    "\n",
    "#### 3. **Feed-Forward Networks**\n",
    "- **Purpose**: Add non-linearity and computational capacity between attention layers\n",
    "- **Architecture**: Two linear transformations with ReLU activation\n",
    "- **Role**: Processes the attended representations to extract higher-level features\n",
    "\n",
    "#### 4. **Layer Normalization & Residual Connections**\n",
    "- **Stability**: Normalizes inputs to prevent gradient explosion/vanishing\n",
    "- **Skip connections**: Allow gradients to flow directly to earlier layers\n",
    "- **Training efficiency**: Enables training of deeper networks\n",
    "\n",
    "#### 5. **Masked Self-Attention**\n",
    "- **Causal masking**: Prevents the model from \"cheating\" by looking at future tokens\n",
    "- **Autoregressive generation**: Essential for generating sequences one token at a time\n",
    "\n",
    "Let's implement each component step by step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Add positional information to token embeddings using sinusoidal functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_length=1000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create the div_term for sinusoidal functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices and cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len].to(x.device)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for queries, keys, values, and output\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # 1. Generate Q, K, V matrices\n",
    "        Q = self.w_q(x)  # (batch_size, seq_len, d_model)\n",
    "        K = self.w_k(x)\n",
    "        V = self.w_v(x)\n",
    "        \n",
    "        # 2. Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 3. Compute attention\n",
    "        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 4. Concatenate heads and put through final linear layer\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model)\n",
    "        \n",
    "        return self.w_o(attention_output)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"Compute scaled dot-product attention.\"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # Apply mask if provided (for causal attention)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        return torch.matmul(attention_weights, V)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(F.relu(self.linear_1(x))))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer block with self-attention and feed-forward.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the components\n",
    "print(\"=== Testing Transformer Components ===\")\n",
    "\n",
    "# Test dimensions\n",
    "d_model = 128\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Test positional encoding\n",
    "pos_enc = PositionalEncoding(d_model, max_length=50)\n",
    "test_input = torch.randn(batch_size, seq_len, d_model)\n",
    "pos_encoded = pos_enc(test_input)\n",
    "print(f\"Positional encoding: {test_input.shape} → {pos_encoded.shape}\")\n",
    "\n",
    "# Test multi-head attention\n",
    "mha = MultiHeadAttention(d_model, num_heads=8)\n",
    "attn_output = mha(pos_encoded)\n",
    "print(f\"Multi-head attention: {pos_encoded.shape} → {attn_output.shape}\")\n",
    "\n",
    "# Test transformer block\n",
    "transformer_block = TransformerBlock(d_model, num_heads=8, d_ff=512)\n",
    "block_output = transformer_block(pos_encoded)\n",
    "print(f\"Transformer block: {pos_encoded.shape} → {block_output.shape}\")\n",
    "\n",
    "print(\"✅ All components working correctly!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Complete Transformer Model\n",
    "\n",
    "Now let's combine all components into a complete transformer model for our arithmetic task. Our model will include:\n",
    "\n",
    "1. **Token Embedding**: Convert token IDs to dense vector representations\n",
    "2. **Positional Encoding**: Add position information to embeddings\n",
    "3. **Transformer Blocks**: Stack multiple attention and feed-forward layers\n",
    "4. **Output Projection**: Convert hidden states back to vocabulary predictions\n",
    "5. **Causal Masking**: Ensure autoregressive generation during training\n",
    "\n",
    "The model architecture follows the decoder-only approach (similar to GPT), which is perfect for our sequence generation task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArithmeticTransformer(nn.Module):\n",
    "    \"\"\"Complete Transformer model for arithmetic sequence generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=8, num_layers=4, \n",
    "                 d_ff=512, max_length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Embedding and positional encoding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_length)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "    \n",
    "    def create_causal_mask(self, seq_len):\n",
    "        \"\"\"Create causal mask to prevent looking at future tokens.\"\"\"\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "        return mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dimensions\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # Create causal mask if not provided\n",
    "        if mask is None:\n",
    "            mask = self.create_causal_mask(seq_len).to(x.device)\n",
    "        \n",
    "        # Token embedding + positional encoding\n",
    "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, mask)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate(self, tokenizer, prompt=\"\", max_length=20, temperature=1.0):\n",
    "        \"\"\"Generate text given a prompt.\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        # Encode prompt\n",
    "        if prompt:\n",
    "            input_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        else:\n",
    "            input_ids = [tokenizer.sos_id]  # Start with SOS token\n",
    "        \n",
    "        input_tensor = torch.tensor([input_ids], dtype=torch.long)\n",
    "        \n",
    "        generated = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                # Get predictions\n",
    "                logits = self.forward(input_tensor)\n",
    "                \n",
    "                # Get logits for the last token\n",
    "                next_token_logits = logits[0, -1, :] / temperature\n",
    "                \n",
    "                # Sample next token\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                # Stop if EOS token is generated\n",
    "                if next_token == tokenizer.eos_id:\n",
    "                    break\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                \n",
    "                # Add to input for next iteration\n",
    "                input_tensor = torch.cat([\n",
    "                    input_tensor, \n",
    "                    torch.tensor([[next_token]], dtype=torch.long)\n",
    "                ], dim=1)\n",
    "        \n",
    "        return tokenizer.decode(generated, remove_special_tokens=True)\n",
    "\n",
    "# Initialize the model\n",
    "model_config = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'd_model': 128,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 4,\n",
    "    'd_ff': 512,\n",
    "    'max_length': train_dataset.max_length,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "model = ArithmeticTransformer(**model_config)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"=== Model Architecture ===\")\n",
    "print(f\"Model: ArithmeticTransformer\")\n",
    "print(f\"Parameters: {model_config}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\n=== Testing Forward Pass ===\")\n",
    "sample_input = sample_batch['input'][:4]  # Take first 4 examples\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(sample_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output logits range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "\n",
    "print(\"✅ Model forward pass successful!\")\n",
    "\n",
    "# Test generation\n",
    "print(f\"\\n=== Testing Generation ===\")\n",
    "for prompt in [\"3+\", \"5-\", \"2+\"]:\n",
    "    generated = model.generate(tokenizer, prompt=prompt, max_length=10, temperature=0.8)\n",
    "    print(f\"Prompt: '{prompt}' → Generated: '{prompt + generated}'\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Training the Transformer\n",
    "\n",
    "### Training Strategy\n",
    "Now we'll train our transformer to learn arithmetic operations. Our training approach includes:\n",
    "\n",
    "#### **Loss Function**: Cross-Entropy Loss\n",
    "- **Purpose**: Measures how well the model predicts the next token in sequence\n",
    "- **Teacher Forcing**: During training, we provide the correct previous tokens as input\n",
    "- **Autoregressive Learning**: Model learns to predict each next character given the previous context\n",
    "\n",
    "#### **Optimization**: Adam Optimizer with Learning Rate Scheduling\n",
    "- **Adam**: Adaptive learning rates with momentum for stable convergence\n",
    "- **Learning Rate Scheduling**: Reduce learning rate when validation loss plateaus\n",
    "- **Gradient Clipping**: Prevent gradient explosion in deep networks\n",
    "\n",
    "#### **Training Loop Components**:\n",
    "1. **Forward Pass**: Compute predictions for next tokens\n",
    "2. **Loss Calculation**: Compare predictions with ground truth\n",
    "3. **Backward Pass**: Compute gradients via backpropagation  \n",
    "4. **Parameter Update**: Apply gradients to model weights\n",
    "5. **Validation**: Monitor performance on unseen data\n",
    "\n",
    "#### **Metrics We'll Track**:\n",
    "- **Training Loss**: How well the model fits the training data\n",
    "- **Validation Loss**: How well the model generalizes to new data\n",
    "- **Accuracy**: Percentage of correct next-token predictions\n",
    "- **Sequence Accuracy**: Percentage of completely correct arithmetic expressions\n",
    "\n",
    "Let's implement the training loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, tokenizer, criterion, device):\n",
    "    \"\"\"Evaluate model performance on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    correct_sequences = 0\n",
    "    total_sequences = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['input'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # Calculate loss (ignore padding tokens)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "            # Calculate token-level accuracy\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            mask = targets != tokenizer.pad_id\n",
    "            correct_tokens += ((predictions == targets) & mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "            \n",
    "            # Calculate sequence-level accuracy\n",
    "            for i in range(inputs.size(0)):\n",
    "                pred_seq = predictions[i]\n",
    "                target_seq = targets[i]\n",
    "                \n",
    "                # Remove padding tokens\n",
    "                pred_clean = pred_seq[target_seq != tokenizer.pad_id]\n",
    "                target_clean = target_seq[target_seq != tokenizer.pad_id]\n",
    "                \n",
    "                if torch.equal(pred_clean, target_clean):\n",
    "                    correct_sequences += 1\n",
    "                total_sequences += 1\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    sequence_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n",
    "    \n",
    "    return avg_loss, token_accuracy, sequence_accuracy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, tokenizer, num_epochs=50, \n",
    "                learning_rate=0.001, device='cpu'):\n",
    "    \"\"\"Train the transformer model.\"\"\"\n",
    "    \n",
    "    print(f\"Training on device: {device}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_token_acc': [], 'val_token_acc': [],\n",
    "        'train_seq_acc': [], 'val_seq_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 10\n",
    "    \n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Starting training for {num_epochs} epochs\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_tokens_correct = 0\n",
    "        train_tokens_total = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            inputs = batch['input'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            mask = targets != tokenizer.pad_id\n",
    "            train_tokens_correct += ((predictions == targets) & mask).sum().item()\n",
    "            train_tokens_total += mask.sum().item()\n",
    "        \n",
    "        # Calculate average training metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_token_acc = train_tokens_correct / train_tokens_total\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_token_acc, val_seq_acc = evaluate_model(\n",
    "            model, val_loader, tokenizer, criterion, device)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save metrics\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_token_acc'].append(train_token_acc)\n",
    "        history['val_token_acc'].append(val_token_acc)\n",
    "        history['train_seq_acc'].append(0)  # We'll calculate this separately if needed\n",
    "        history['val_seq_acc'].append(val_seq_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Val Token Acc: {val_token_acc:.3f} | \"\n",
    "                  f\"Val Seq Acc: {val_seq_acc:.3f} | \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"\\\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\\\nTraining completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"TRAINING PHASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trained_model, training_history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.001,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Model Evaluation and Analysis\n",
    "\n",
    "### Comprehensive Testing\n",
    "Now that our model is trained, let's thoroughly evaluate its performance on arithmetic tasks. We'll test:\n",
    "\n",
    "#### **1. Quantitative Metrics**\n",
    "- **Training curves**: Visualize loss and accuracy over time\n",
    "- **Final performance**: Token-level and sequence-level accuracy on validation set\n",
    "- **Generalization**: How well the model handles expressions it hasn't seen\n",
    "\n",
    "#### **2. Qualitative Analysis**\n",
    "- **Generation samples**: Examine the model's generated arithmetic expressions\n",
    "- **Error analysis**: Understand what types of mistakes the model makes\n",
    "- **Pattern recognition**: See if the model learned mathematical relationships\n",
    "\n",
    "#### **3. Interactive Testing**\n",
    "- **Custom prompts**: Test the model on user-defined arithmetic problems\n",
    "- **Edge cases**: Evaluate behavior on boundary conditions\n",
    "- **Creativity**: Observe if the model can generate valid new expressions\n",
    "\n",
    "Let's start with visualization and comprehensive evaluation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(training_history['train_loss'], label='Training Loss', color='blue')\n",
    "plt.plot(training_history['val_loss'], label='Validation Loss', color='red')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Token accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(training_history['train_token_acc'], label='Training Token Acc', color='blue')\n",
    "plt.plot(training_history['val_token_acc'], label='Validation Token Acc', color='red')\n",
    "plt.title('Token-Level Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Sequence accuracy\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(training_history['val_seq_acc'], label='Validation Sequence Acc', color='red')\n",
    "plt.title('Sequence-Level Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation on validation set\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)\n",
    "final_val_loss, final_token_acc, final_seq_acc = evaluate_model(\n",
    "    trained_model, val_loader, tokenizer, criterion, device)\n",
    "\n",
    "print(f\"Final Validation Results:\")\n",
    "print(f\"  Loss: {final_val_loss:.4f}\")\n",
    "print(f\"  Token Accuracy: {final_token_acc:.3f} ({final_token_acc*100:.1f}%)\")\n",
    "print(f\"  Sequence Accuracy: {final_seq_acc:.3f} ({final_seq_acc*100:.1f}%)\")\n",
    "\n",
    "# Test on training set for comparison\n",
    "final_train_loss, final_train_token_acc, final_train_seq_acc = evaluate_model(\n",
    "    trained_model, train_loader, tokenizer, criterion, device)\n",
    "\n",
    "print(f\"\\\\nFinal Training Results:\")\n",
    "print(f\"  Loss: {final_train_loss:.4f}\")\n",
    "print(f\"  Token Accuracy: {final_train_token_acc:.3f} ({final_train_token_acc*100:.1f}%)\")\n",
    "print(f\"  Sequence Accuracy: {final_train_seq_acc:.3f} ({final_train_seq_acc*100:.1f}%)\")\n",
    "\n",
    "# Generation testing\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"GENERATION TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def test_arithmetic_generation(model, tokenizer, test_cases, device):\n",
    "    \"\"\"Test model on specific arithmetic problems.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for prompt in test_cases:\n",
    "        # Generate multiple samples for diversity\n",
    "        generations = []\n",
    "        for temp in [0.1, 0.5, 1.0]:  # Different temperatures\n",
    "            generated = model.generate(tokenizer, prompt=prompt, \n",
    "                                     max_length=15, temperature=temp)\n",
    "            full_expr = prompt + generated\n",
    "            generations.append((temp, full_expr))\n",
    "        \n",
    "        results.append((prompt, generations))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test cases covering different scenarios\n",
    "test_cases = [\n",
    "    \"0+\", \"1+\", \"2+\", \"3+\", \"4+\", \"5+\",  # Addition\n",
    "    \"5-\", \"7-\", \"9-\", \"6-\", \"8-\",        # Subtraction  \n",
    "    \"2+3\", \"4+5\", \"7+2\",                 # Partial expressions\n",
    "    \"9+9\", \"8+7\", \"6+8\",                 # Larger sums\n",
    "]\n",
    "\n",
    "# Run generation tests\n",
    "generation_results = test_arithmetic_generation(trained_model, tokenizer, test_cases, device)\n",
    "\n",
    "print(\"Generation Test Results:\")\n",
    "print(\"-\" * 80)\n",
    "for prompt, generations in generation_results:\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    for temp, full_expr in generations:\n",
    "        print(f\"  T={temp}: {full_expr}\")\n",
    "    print()\n",
    "\n",
    "# Detailed analysis of specific examples\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def analyze_arithmetic_correctness(expressions):\n",
    "    \"\"\"Analyze the mathematical correctness of generated expressions.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errors = []\n",
    "    \n",
    "    for expr in expressions:\n",
    "        total += 1\n",
    "        try:\n",
    "            # Parse the expression\n",
    "            if '=' in expr:\n",
    "                left, right = expr.split('=')\n",
    "                \n",
    "                if '+' in left:\n",
    "                    operand1, operand2 = left.split('+')\n",
    "                    expected = int(operand1) + int(operand2)\n",
    "                elif '-' in left:\n",
    "                    operand1, operand2 = left.split('-')\n",
    "                    expected = int(operand1) - int(operand2)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                actual = int(right)\n",
    "                \n",
    "                if actual == expected:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    errors.append(f\"{expr} (expected {expected})\")\n",
    "            \n",
    "        except (ValueError, IndexError):\n",
    "            errors.append(f\"{expr} (parsing error)\")\n",
    "    \n",
    "    return correct, total, errors\n",
    "\n",
    "# Collect all generated expressions\n",
    "all_expressions = []\n",
    "for prompt, generations in generation_results:\n",
    "    for temp, full_expr in generations:\n",
    "        if '=' in full_expr:\n",
    "            all_expressions.append(full_expr)\n",
    "\n",
    "# Analyze correctness\n",
    "correct, total, errors = analyze_arithmetic_correctness(all_expressions)\n",
    "\n",
    "print(f\"Mathematical Correctness Analysis:\")\n",
    "print(f\"  Correct expressions: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "print(f\"  Errors found: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\\\nFirst 10 errors:\")\n",
    "    for i, error in enumerate(errors[:10]):\n",
    "        print(f\"  {i+1}. {error}\")\n",
    "\n",
    "# Test with completely novel expressions\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"NOVEL EXPRESSION TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "novel_prompts = [\"1+7\", \"9-4\", \"6+6\", \"8-8\", \"4+4\"]\n",
    "print(\"Testing on expressions not in training data:\")\n",
    "\n",
    "for prompt in novel_prompts:\n",
    "    # Check if this expression was in our training data\n",
    "    full_expected = None\n",
    "    for expr in data:\n",
    "        if expr.startswith(prompt):\n",
    "            full_expected = expr\n",
    "            break\n",
    "    \n",
    "    generated = trained_model.generate(tokenizer, prompt=prompt, \n",
    "                                     max_length=10, temperature=0.1)\n",
    "    full_generated = prompt + generated\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {full_generated}\")\n",
    "    print(f\"In training: {'Yes' if full_expected else 'No'}\")\n",
    "    if full_expected:\n",
    "        print(f\"Expected: {full_expected}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\\\n🎉 Tutorial completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
