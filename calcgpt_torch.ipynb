{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CalcGPT: Learning Arithmetic with PyTorch\n",
    "\n",
    "## Project Overview and Objectives\n",
    "\n",
    "### What are we building?\n",
    "In this comprehensive tutorial, we'll implement a **Transformer model from scratch** using PyTorch to solve a fascinating problem: teaching a neural network to perform basic arithmetic operations. Our model will learn to understand and generate mathematical expressions involving addition and subtraction.\n",
    "\n",
    "### Learning Goals\n",
    "By the end of this tutorial, you will have:\n",
    "- **Built a complete Transformer architecture** with multi-head attention, positional encoding, and feed-forward layers\n",
    "- **Understood sequence-to-sequence learning** in the context of mathematical reasoning\n",
    "- **Implemented a custom tokenizer** for mathematical expressions\n",
    "- **Trained and evaluated** a neural network on symbolic reasoning tasks\n",
    "- **Gained hands-on experience** with attention mechanisms and their role in learning structured patterns\n",
    "\n",
    "### The Dataset Challenge\n",
    "We'll use data from the `data/calcgpt-*.txt` family which contains simple arithmetic equations like:\n",
    "- `3+4=7` (addition problems)\n",
    "- `8-5=3` (subtraction problems)\n",
    "\n",
    "This seemingly simple task is actually quite challenging for neural networks because it requires:\n",
    "1. **Understanding symbolic relationships** between numbers and operators\n",
    "2. **Learning mathematical rules** rather than just pattern matching\n",
    "3. **Generalizing arithmetic operations** across different number combinations\n",
    "4. **Sequential reasoning** to process left-to-right mathematical expressions\n",
    "\n",
    "### Why Transformers for Arithmetic?\n",
    "Transformers excel at this task because:\n",
    "- **Attention mechanisms** can learn to focus on relevant parts of the equation\n",
    "- **Self-attention** helps the model understand relationships between operands and operators  \n",
    "- **Positional encoding** maintains the order of mathematical operations\n",
    "- **Parallel processing** allows efficient training on sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detecting optimal device for training...\n",
      "‚úÖ Apple MPS detected\n",
      "üçé Using Metal Performance Shaders for GPU acceleration\n",
      "üì± Optimized for Apple Silicon (M1/M2/M3)\n",
      "‚ÑπÔ∏è  Note: Mixed precision training will use CPU fallback when needed\n",
      "üé≤ Random seeds set for reproducibility on MPS\n",
      "üéØ Selected device: mps\n",
      "\n",
      "üî• Mixed Precision Training: ‚úÖ Supported\n",
      "‚ö° Training will use automatic mixed precision for faster computation and lower memory usage\n",
      "\n",
      "============================================================\n",
      "GPU OPTIMIZATION SETUP COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_optimal_device():\n",
    "    \"\"\"\n",
    "    Detect and configure the optimal device for training with comprehensive GPU setup.\n",
    "    Returns device and provides detailed hardware information.\n",
    "    \"\"\"\n",
    "    print(\"üîç Detecting optimal device for training...\")\n",
    "    \n",
    "    # Check for CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        print(f\"‚úÖ CUDA detected: {gpu_name}\")\n",
    "        print(f\"üìä GPU Memory: {gpu_memory:.1f} GB\")\n",
    "        print(f\"üî¢ CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"‚ö° cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "        \n",
    "        # Enable cuDNN benchmark for consistent input sizes (optimization)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"üöÄ cuDNN benchmark mode enabled for consistent input sizes\")\n",
    "        \n",
    "        # Check if Tensor Cores are available\n",
    "        if torch.cuda.get_device_capability(0)[0] >= 7:\n",
    "            print(\"‚ö° Tensor Cores available - Mixed precision training will be highly optimized\")\n",
    "        \n",
    "        # Memory optimization settings\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"üßπ GPU memory cache cleared\")\n",
    "        \n",
    "    # Check for Apple Metal Performance Shaders (MPS)\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(\"‚úÖ Apple MPS detected\")\n",
    "        print(\"üçé Using Metal Performance Shaders for GPU acceleration\")\n",
    "        print(\"üì± Optimized for Apple Silicon (M1/M2/M3)\")\n",
    "        \n",
    "        # MPS doesn't support all CUDA features, but still very fast\n",
    "        print(\"‚ÑπÔ∏è  Note: Mixed precision training will use CPU fallback when needed\")\n",
    "        \n",
    "    # Fallback to CPU\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        cpu_count = os.cpu_count()\n",
    "        ram_gb = psutil.virtual_memory().total / 1024**3\n",
    "        \n",
    "        print(\"üíª Using CPU for training\")\n",
    "        print(f\"üî¢ CPU Cores: {cpu_count}\")\n",
    "        print(f\"üíæ RAM: {ram_gb:.1f} GB\")\n",
    "        print(\"‚ö†Ô∏è  Training will be slower - consider using a GPU for large models\")\n",
    "    \n",
    "    # Set optimal number of threads for PyTorch\n",
    "    if device.type == 'cpu':\n",
    "        torch.set_num_threads(min(8, os.cpu_count()))\n",
    "        print(f\"üßµ PyTorch threads set to: {torch.get_num_threads()}\")\n",
    "    \n",
    "    # Set random seeds for reproducibility across all device types\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    elif device.type == 'mps':\n",
    "        torch.mps.manual_seed(42)\n",
    "    \n",
    "    print(f\"üé≤ Random seeds set for reproducibility on {device.type.upper()}\")\n",
    "    print(f\"üéØ Selected device: {device}\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Initialize optimal device\n",
    "device = get_optimal_device()\n",
    "\n",
    "# Check if mixed precision training is supported\n",
    "mixed_precision_supported = device.type == 'cuda' or (\n",
    "    device.type == 'mps' and hasattr(torch.backends.mps, 'is_available')\n",
    ")\n",
    "\n",
    "print(f\"\\nüî• Mixed Precision Training: {'‚úÖ Supported' if mixed_precision_supported else '‚ùå Not supported'}\")\n",
    "\n",
    "if mixed_precision_supported:\n",
    "    print(\"‚ö° Training will use automatic mixed precision for faster computation and lower memory usage\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU OPTIMIZATION SETUP COMPLETE\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Data Loading and Exploration\n",
    "\n",
    "### Understanding Our Dataset\n",
    "Before we can train our transformer, we need to understand exactly what we're working with. Our dataset consists of simple arithmetic expressions in the format `operand1 operator operand2 = result`. \n",
    "\n",
    "### Why Data Loading Matters\n",
    "Proper data loading is crucial because:\n",
    "- **Quality assessment**: We need to verify our data is clean and consistent\n",
    "- **Pattern recognition**: Understanding the data structure helps us design appropriate tokenization\n",
    "- **Training efficiency**: Knowing dataset size helps us plan batch sizes and epochs\n",
    "- **Debugging**: If our model fails, we first check if the data is correctly loaded\n",
    "\n",
    "Let's load our arithmetic dataset and examine its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file 'data/calcgpt-100000_d0-2.txt'...\n",
      "Loaded 16041 lines from 'data/calcgpt-100000_d0-2.txt'\n",
      "\n",
      "=== Data Exploration ===\n",
      "First 10 examples:\n",
      "   1: 0+0=0\n",
      "   2: 0-0=0\n",
      "   3: 0+1=1\n",
      "   4: 0+2=2\n",
      "   5: 0+10=10\n",
      "   6: 0+11=11\n",
      "   7: 0+12=12\n",
      "   8: 0+20=20\n",
      "   9: 0+21=21\n",
      "  10: 0+22=22\n",
      "\n",
      "Last 5 examples:\n",
      "  16037: 100000+22220=122220\n",
      "  16038: 100000+22221=122221\n",
      "  16039: 100000+22222=122222\n",
      "  16040: 100000+100000=200000\n",
      "  16041: 100000-100000=0\n",
      "\n",
      "=== Data Analysis ===\n",
      "Operations distribution: Counter({'+': 8263, '-': 7778})\n",
      "Operand range: 0 to 100000\n",
      "Result range: 0 to 200000\n",
      "Unique operands: [0, 1, 2, 10, 11, 12, 20, 21, 22, 100, 101, 102, 110, 111, 112, 120, 121, 122, 200, 201, 202, 210, 211, 212, 220, 221, 222, 1000, 1001, 1002, 1010, 1011, 1012, 1020, 1021, 1022, 1100, 1101, 1102, 1110, 1111, 1112, 1120, 1121, 1122, 1200, 1201, 1202, 1210, 1211, 1212, 1220, 1221, 1222, 2000, 2001, 2002, 2010, 2011, 2012, 2020, 2021, 2022, 2100, 2101, 2102, 2110, 2111, 2112, 2120, 2121, 2122, 2200, 2201, 2202, 2210, 2211, 2212, 2220, 2221, 2222, 10000, 10001, 10002, 10010, 10011, 10012, 10020, 10021, 10022, 10100, 10101, 10102, 10110, 10111, 10112, 10120, 10121, 10122, 10200, 10201, 10202, 10210, 10211, 10212, 10220, 10221, 10222, 11000, 11001, 11002, 11010, 11011, 11012, 11020, 11021, 11022, 11100, 11101, 11102, 11110, 11111, 11112, 11120, 11121, 11122, 11200, 11201, 11202, 11210, 11211, 11212, 11220, 11221, 11222, 12000, 12001, 12002, 12010, 12011, 12012, 12020, 12021, 12022, 12100, 12101, 12102, 12110, 12111, 12112, 12120, 12121, 12122, 12200, 12201, 12202, 12210, 12211, 12212, 12220, 12221, 12222, 20000, 20001, 20002, 20010, 20011, 20012, 20020, 20021, 20022, 20100, 20101, 20102, 20110, 20111, 20112, 20120, 20121, 20122, 20200, 20201, 20202, 20210, 20211, 20212, 20220, 20221, 20222, 21000, 21001, 21002, 21010, 21011, 21012, 21020, 21021, 21022, 21100, 21101, 21102, 21110, 21111, 21112, 21120, 21121, 21122, 21200, 21201, 21202, 21210, 21211, 21212, 21220, 21221, 21222, 22000, 22001, 22002, 22010, 22011, 22012, 22020, 22021, 22022, 22100, 22101, 22102, 22110, 22111, 22112, 22120, 22121, 22122, 22200, 22201, 22202, 22210, 22211, 22212, 22220, 22221, 22222, 100000]\n",
      "Unique results: [0, 1, 2, 10, 11, 12, 20, 21, 22, 100, 101, 102, 110, 111, 112, 120, 121, 122, 200, 201, 202, 210, 211, 212, 220, 221, 222, 1000, 1001, 1002, 1010, 1011, 1012, 1020, 1021, 1022, 1100, 1101, 1102, 1110, 1111, 1112, 1120, 1121, 1122, 1200, 1201, 1202, 1210, 1211, 1212, 1220, 1221, 1222, 2000, 2001, 2002, 2010, 2011, 2012, 2020, 2021, 2022, 2100, 2101, 2102, 2110, 2111, 2112, 2120, 2121, 2122, 2200, 2201, 2202, 2210, 2211, 2212, 2220, 2221, 2222, 10000, 10001, 10002, 10010, 10011, 10012, 10020, 10021, 10022, 10100, 10101, 10102, 10110, 10111, 10112, 10120, 10121, 10122, 10200, 10201, 10202, 10210, 10211, 10212, 10220, 10221, 10222, 11000, 11001, 11002, 11010, 11011, 11012, 11020, 11021, 11022, 11100, 11101, 11102, 11110, 11111, 11112, 11120, 11121, 11122, 11200, 11201, 11202, 11210, 11211, 11212, 11220, 11221, 11222, 12000, 12001, 12002, 12010, 12011, 12012, 12020, 12021, 12022, 12100, 12101, 12102, 12110, 12111, 12112, 12120, 12121, 12122, 12200, 12201, 12202, 12210, 12211, 12212, 12220, 12221, 12222, 20000, 20001, 20002, 20010, 20011, 20012, 20020, 20021, 20022, 20100, 20101, 20102, 20110, 20111, 20112, 20120, 20121, 20122, 20200, 20201, 20202, 20210, 20211, 20212, 20220, 20221, 20222, 21000, 21001, 21002, 21010, 21011, 21012, 21020, 21021, 21022, 21100, 21101, 21102, 21110, 21111, 21112, 21120, 21121, 21122, 21200, 21201, 21202, 21210, 21211, 21212, 21220, 21221, 21222, 22000, 22001, 22002, 22010, 22011, 22012, 22020, 22021, 22022, 22100, 22101, 22102, 22110, 22111, 22112, 22120, 22121, 22122, 22200, 22201, 22202, 22210, 22211, 22212, 22220, 22221, 22222, 100000, 100001, 100002, 100010, 100011, 100012, 100020, 100021, 100022, 100100, 100101, 100102, 100110, 100111, 100112, 100120, 100121, 100122, 100200, 100201, 100202, 100210, 100211, 100212, 100220, 100221, 100222, 101000, 101001, 101002, 101010, 101011, 101012, 101020, 101021, 101022, 101100, 101101, 101102, 101110, 101111, 101112, 101120, 101121, 101122, 101200, 101201, 101202, 101210, 101211, 101212, 101220, 101221, 101222, 102000, 102001, 102002, 102010, 102011, 102012, 102020, 102021, 102022, 102100, 102101, 102102, 102110, 102111, 102112, 102120, 102121, 102122, 102200, 102201, 102202, 102210, 102211, 102212, 102220, 102221, 102222, 110000, 110001, 110002, 110010, 110011, 110012, 110020, 110021, 110022, 110100, 110101, 110102, 110110, 110111, 110112, 110120, 110121, 110122, 110200, 110201, 110202, 110210, 110211, 110212, 110220, 110221, 110222, 111000, 111001, 111002, 111010, 111011, 111012, 111020, 111021, 111022, 111100, 111101, 111102, 111110, 111111, 111112, 111120, 111121, 111122, 111200, 111201, 111202, 111210, 111211, 111212, 111220, 111221, 111222, 112000, 112001, 112002, 112010, 112011, 112012, 112020, 112021, 112022, 112100, 112101, 112102, 112110, 112111, 112112, 112120, 112121, 112122, 112200, 112201, 112202, 112210, 112211, 112212, 112220, 112221, 112222, 120000, 120001, 120002, 120010, 120011, 120012, 120020, 120021, 120022, 120100, 120101, 120102, 120110, 120111, 120112, 120120, 120121, 120122, 120200, 120201, 120202, 120210, 120211, 120212, 120220, 120221, 120222, 121000, 121001, 121002, 121010, 121011, 121012, 121020, 121021, 121022, 121100, 121101, 121102, 121110, 121111, 121112, 121120, 121121, 121122, 121200, 121201, 121202, 121210, 121211, 121212, 121220, 121221, 121222, 122000, 122001, 122002, 122010, 122011, 122012, 122020, 122021, 122022, 122100, 122101, 122102, 122110, 122111, 122112, 122120, 122121, 122122, 122200, 122201, 122202, 122210, 122211, 122212, 122220, 122221, 122222, 200000]\n",
      "Expression length range: 5 to 20 characters\n",
      "Average expression length: 15.0 characters\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data/calcgpt-100_d0-1.txt\"\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"Load arithmetic expressions from file and clean them.\"\"\"\n",
    "    with open(file_name, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Clean the data by removing whitespace and empty lines\n",
    "    cleaned_data = [line.strip() for line in lines if line.strip()]\n",
    "    return cleaned_data\n",
    "\n",
    "print(\"Loading file '{}'...\".format(data_file))\n",
    "data = load_data(data_file)\n",
    "print(\"Loaded {} lines from '{}'\".format(len(data), data_file))\n",
    "\n",
    "# Let's explore our data\n",
    "print(\"\\n=== Data Exploration ===\")\n",
    "print(\"First 10 examples:\")\n",
    "for i, example in enumerate(data[:10]):\n",
    "    print(f\"  {i+1:2d}: {example}\")\n",
    "\n",
    "print(f\"\\nLast 5 examples:\")\n",
    "for i, example in enumerate(data[-5:], len(data)-4):\n",
    "    print(f\"  {i:2d}: {example}\")\n",
    "\n",
    "# Analyze the structure\n",
    "print(f\"\\n=== Data Analysis ===\")\n",
    "operations = []\n",
    "operands = []\n",
    "results = []\n",
    "\n",
    "for expr in data:\n",
    "    # Parse each expression\n",
    "    if '+' in expr:\n",
    "        left, right_and_result = expr.split('+')\n",
    "        operator = '+'\n",
    "        right, result = right_and_result.split('=')\n",
    "    elif '-' in expr:\n",
    "        left, right_and_result = expr.split('-')\n",
    "        operator = '-'\n",
    "        right, result = right_and_result.split('=')\n",
    "    \n",
    "    operations.append(operator)\n",
    "    operands.extend([int(left), int(right)])\n",
    "    results.append(int(result))\n",
    "\n",
    "print(f\"Operations distribution: {Counter(operations)}\")\n",
    "print(f\"Operand range: {min(operands)} to {max(operands)}\")\n",
    "print(f\"Result range: {min(results)} to {max(results)}\")\n",
    "print(f\"Unique operands: {sorted(set(operands))}\")\n",
    "print(f\"Unique results: {sorted(set(results))}\")\n",
    "\n",
    "# Calculate expression lengths\n",
    "expr_lengths = [len(expr) for expr in data]\n",
    "print(f\"Expression length range: {min(expr_lengths)} to {max(expr_lengths)} characters\")\n",
    "print(f\"Average expression length: {np.mean(expr_lengths):.1f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Vocabulary and Tokenization\n",
    "\n",
    "### The Foundation of Language Models\n",
    "Before our transformer can process mathematical expressions, we need to convert text into numbers that the neural network can understand. This process involves two key steps:\n",
    "1. **Tokenization**: Breaking expressions into individual tokens (characters/symbols)\n",
    "2. **Vocabulary mapping**: Creating bidirectional mappings between tokens and integer IDs\n",
    "\n",
    "### Why Character-Level Tokenization?\n",
    "For our arithmetic task, we'll use character-level tokenization because:\n",
    "- **Simplicity**: Each character (0-9, +, -, =) is a meaningful unit\n",
    "- **Completeness**: We can represent any arithmetic expression with a small vocabulary\n",
    "- **Generalization**: The model learns fundamental relationships between symbols\n",
    "- **No OOV issues**: Out-of-vocabulary problems are eliminated with character-level tokens\n",
    "\n",
    "### Key Components We'll Create:\n",
    "1. **Token-to-ID mapping**: Convert characters to integers for neural network input\n",
    "2. **ID-to-token mapping**: Convert model outputs back to readable characters  \n",
    "3. **Special tokens**: Add padding, start-of-sequence, and end-of-sequence markers\n",
    "4. **Encoding/decoding functions**: Transform between text and tensor representations\n",
    "\n",
    "Let's build our vocabulary system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created with 9 tokens:\n",
      "Tokens: ['<PAD>', '<SOS>', '<EOS>', '+', '-', '0', '1', '2', '=']\n",
      "Special token IDs: PAD=0, SOS=1, EOS=2\n",
      "\n",
      "=== Tokenizer Testing ===\n",
      "'0+0=0' ‚Üí [1, 5, 3, 5, 8, 5, 2] ‚Üí '0+0=0'\n",
      "'0-0=0' ‚Üí [1, 5, 4, 5, 8, 5, 2] ‚Üí '0-0=0'\n",
      "'0+1=1' ‚Üí [1, 5, 3, 6, 8, 6, 2] ‚Üí '0+1=1'\n",
      "'0+2=2' ‚Üí [1, 5, 3, 7, 8, 7, 2] ‚Üí '0+2=2'\n",
      "'0+10=10' ‚Üí [1, 5, 3, 6, 5, 8, 6, 5, 2] ‚Üí '0+10=10'\n"
     ]
    }
   ],
   "source": [
    "class ArithmeticTokenizer:\n",
    "    \"\"\"A character-level tokenizer for arithmetic expressions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Extract all unique characters from our data\n",
    "        all_chars = set()\n",
    "        for expr in data:\n",
    "            all_chars.update(expr)\n",
    "        \n",
    "        # Define special tokens\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.SOS_TOKEN = '<SOS>'  # Start of sequence\n",
    "        self.EOS_TOKEN = '<EOS>'  # End of sequence\n",
    "        \n",
    "        # Create vocabulary: special tokens + regular characters\n",
    "        self.vocab = [self.PAD_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN] + sorted(all_chars)\n",
    "        \n",
    "        # Create mappings\n",
    "        self.char_to_id = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        self.id_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
    "        \n",
    "        # Store important indices\n",
    "        self.pad_id = self.char_to_id[self.PAD_TOKEN]\n",
    "        self.sos_id = self.char_to_id[self.SOS_TOKEN]\n",
    "        self.eos_id = self.char_to_id[self.EOS_TOKEN]\n",
    "        \n",
    "        print(f\"Vocabulary created with {len(self.vocab)} tokens:\")\n",
    "        print(f\"Tokens: {self.vocab}\")\n",
    "        print(f\"Special token IDs: PAD={self.pad_id}, SOS={self.sos_id}, EOS={self.eos_id}\")\n",
    "    \n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        \"\"\"Convert text to list of token IDs.\"\"\"\n",
    "        if add_special_tokens:\n",
    "            ids = [self.sos_id]\n",
    "            ids.extend([self.char_to_id[char] for char in text])\n",
    "            ids.append(self.eos_id)\n",
    "        else:\n",
    "            ids = [self.char_to_id[char] for char in text]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids, remove_special_tokens=True):\n",
    "        \"\"\"Convert list of token IDs back to text.\"\"\"\n",
    "        chars = [self.id_to_char[id] for id in ids]\n",
    "        if remove_special_tokens:\n",
    "            # Remove special tokens\n",
    "            chars = [char for char in chars if char not in [self.PAD_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN]]\n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "# Create our tokenizer\n",
    "tokenizer = ArithmeticTokenizer()\n",
    "\n",
    "# Test the tokenizer\n",
    "print(f\"\\n=== Tokenizer Testing ===\")\n",
    "for expr in data[:5]:\n",
    "    encoded = tokenizer.encode(expr)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(f\"'{expr}' ‚Üí {encoded} ‚Üí '{decoded}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Dataset Creation and Preprocessing\n",
    "\n",
    "### Preparing Data for Training\n",
    "Now that we have our tokenizer, we need to create a PyTorch Dataset that can:\n",
    "1. **Convert expressions to tensors**: Transform text into numerical inputs the model can process\n",
    "2. **Handle variable lengths**: Pad sequences to create uniform batch sizes\n",
    "3. **Create input-output pairs**: For sequence-to-sequence learning, we need both input and target sequences\n",
    "4. **Enable efficient batching**: Organize data for parallel processing\n",
    "\n",
    "### Sequence-to-Sequence Setup\n",
    "For our arithmetic task, we'll use a **teacher forcing** approach during training:\n",
    "- **Input sequence**: The entire expression including the equals sign (`3+4=`)\n",
    "- **Target sequence**: The expression shifted by one position (`+4=7`)\n",
    "- **Prediction task**: Given `3+4=`, predict the next character at each position\n",
    "\n",
    "This setup allows the model to learn the relationship between mathematical operations and their results.\n",
    "\n",
    "### Key Preprocessing Steps:\n",
    "1. **Tokenization**: Convert text to token IDs\n",
    "2. **Padding**: Ensure all sequences have the same length\n",
    "3. **Tensor conversion**: Create PyTorch tensors for efficient computation\n",
    "4. **Train/validation split**: Separate data for training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split: 12832 training, 3209 validation examples\n",
      "Pre-tokenizing expressions for faster training...\n",
      "Dataset created with 12832 expressions\n",
      "Maximum sequence length: 22\n",
      "Pre-tokenizing expressions for faster training...\n",
      "Dataset created with 3209 expressions\n",
      "Maximum sequence length: 22\n",
      "Created optimized data loaders:\n",
      "  Batch size: 64\n",
      "  Training batches: 201\n",
      "  Validation batches: 51\n",
      "  DataLoader settings: {'num_workers': 0, 'pin_memory': False}\n",
      "  üìù Note: num_workers=0 for Jupyter compatibility (prevents multiprocessing errors)\n",
      "\n",
      "=== Dataset Testing ===\n",
      "Batch input shape: torch.Size([64, 21])\n",
      "Batch target shape: torch.Size([64, 21])\n",
      "\n",
      "Example 1:\n",
      "  Original: '20102+2100=22202'\n",
      "  Input:    [1, 7, 5, 6, 5, 7, 3, 7, 6, 5, 5, 8, 7, 7, 7, 5, 7, 0, 0, 0, 0]\n",
      "  Target:   [7, 5, 6, 5, 7, 3, 7, 6, 5, 5, 8, 7, 7, 7, 5, 7, 2, 0, 0, 0, 0]\n",
      "  Input tokens:  ['<SOS>', '2', '0', '1', '0', '2', '+', '2', '1', '0', '0', '=', '2', '2', '2', '0', '2', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "  Target tokens: ['2', '0', '1', '0', '2', '+', '2', '1', '0', '0', '=', '2', '2', '2', '0', '2', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Example 2:\n",
      "  Original: '21010+1=21011'\n",
      "  Input:    [1, 7, 6, 5, 6, 5, 3, 6, 8, 7, 6, 5, 6, 6, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Target:   [7, 6, 5, 6, 5, 3, 6, 8, 7, 6, 5, 6, 6, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Input tokens:  ['<SOS>', '2', '1', '0', '1', '0', '+', '1', '=', '2', '1', '0', '1', '1', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "  Target tokens: ['2', '1', '0', '1', '0', '+', '1', '=', '2', '1', '0', '1', '1', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Example 3:\n",
      "  Original: '10012-10002=10'\n",
      "  Input:    [1, 6, 5, 5, 6, 7, 4, 6, 5, 5, 5, 7, 8, 6, 5, 0, 0, 0, 0, 0, 0]\n",
      "  Target:   [6, 5, 5, 6, 7, 4, 6, 5, 5, 5, 7, 8, 6, 5, 2, 0, 0, 0, 0, 0, 0]\n",
      "  Input tokens:  ['<SOS>', '1', '0', '0', '1', '2', '-', '1', '0', '0', '0', '2', '=', '1', '0', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "  Target tokens: ['1', '0', '0', '1', '2', '-', '1', '0', '0', '0', '2', '=', '1', '0', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "class ArithmeticDataset(Dataset):\n",
    "    \"\"\"Optimized dataset for arithmetic expressions with pre-tokenization.\"\"\"\n",
    "    \n",
    "    def __init__(self, expressions, tokenizer, max_length=None, device=None):\n",
    "        self.expressions = expressions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device or torch.device('cpu')\n",
    "        \n",
    "        # Pre-calculate max length if not provided\n",
    "        if max_length is None:\n",
    "            encoded_lengths = [len(tokenizer.encode(expr)) for expr in expressions]\n",
    "            self.max_length = max(encoded_lengths)\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "        \n",
    "        # Pre-tokenize all expressions for faster loading\n",
    "        print(\"Pre-tokenizing expressions for faster training...\")\n",
    "        self.tokenized_data = []\n",
    "        for expr in expressions:\n",
    "            encoded = tokenizer.encode(expr)\n",
    "            input_seq = encoded[:-1]  # Remove last token for input\n",
    "            target_seq = encoded[1:]  # Remove first token for target\n",
    "            \n",
    "            # Pad sequences to max_length\n",
    "            input_seq = self.pad_sequence(input_seq, self.max_length - 1)\n",
    "            target_seq = self.pad_sequence(target_seq, self.max_length - 1)\n",
    "            \n",
    "            self.tokenized_data.append({\n",
    "                'input': torch.tensor(input_seq, dtype=torch.long),\n",
    "                'target': torch.tensor(target_seq, dtype=torch.long),\n",
    "                'original': expr\n",
    "            })\n",
    "        \n",
    "        print(f\"Dataset created with {len(expressions)} expressions\")\n",
    "        print(f\"Maximum sequence length: {self.max_length}\")\n",
    "        \n",
    "    def pad_sequence(self, seq, max_len):\n",
    "        \"\"\"Pad sequence to max_len with PAD tokens.\"\"\"\n",
    "        if len(seq) >= max_len:\n",
    "            return seq[:max_len]\n",
    "        else:\n",
    "            return seq + [self.tokenizer.pad_id] * (max_len - len(seq))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_data[idx]\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size = len(data) - train_size\n",
    "\n",
    "# Shuffle data before splitting\n",
    "shuffled_data = data.copy()\n",
    "random.shuffle(shuffled_data)\n",
    "\n",
    "train_data = shuffled_data[:train_size]\n",
    "val_data = shuffled_data[train_size:]\n",
    "\n",
    "print(f\"Data split: {train_size} training, {val_size} validation examples\")\n",
    "\n",
    "# Create optimized datasets with device awareness\n",
    "train_dataset = ArithmeticDataset(train_data, tokenizer, device=device)\n",
    "val_dataset = ArithmeticDataset(val_data, tokenizer, max_length=train_dataset.max_length, device=device)\n",
    "\n",
    "# Get optimal DataLoader settings based on device\n",
    "def get_optimal_dataloader_settings(device):\n",
    "    \"\"\"Get optimal DataLoader settings based on device (Jupyter-safe).\"\"\"\n",
    "    if device.type == 'cuda':\n",
    "        return {\n",
    "            'batch_size': 128,  # Larger batch size for better GPU utilization\n",
    "            'num_workers': 0,   # Disabled for Jupyter compatibility\n",
    "            'pin_memory': True, # Faster CPU to GPU transfer\n",
    "            # 'persistent_workers': True  # Only works with num_workers > 0\n",
    "        }\n",
    "    elif device.type == 'mps':\n",
    "        return {\n",
    "            'batch_size': 64,   # Moderate batch size for MPS\n",
    "            'num_workers': 0,   # Disabled for Jupyter compatibility\n",
    "            'pin_memory': False # Not needed for MPS\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'batch_size': 32,   # Smaller batch size for CPU\n",
    "            'num_workers': 0    # No parallel loading on CPU\n",
    "        }\n",
    "\n",
    "loader_settings = get_optimal_dataloader_settings(device)\n",
    "batch_size = loader_settings.pop('batch_size')\n",
    "\n",
    "# Create optimized data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **loader_settings)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, **loader_settings)\n",
    "\n",
    "print(f\"Created optimized data loaders:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  DataLoader settings: {loader_settings}\")\n",
    "print(f\"  üìù Note: num_workers=0 for Jupyter compatibility (prevents multiprocessing errors)\")\n",
    "\n",
    "# Test the dataset\n",
    "print(f\"\\n=== Dataset Testing ===\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch input shape: {sample_batch['input'].shape}\")\n",
    "print(f\"Batch target shape: {sample_batch['target'].shape}\")\n",
    "\n",
    "# Show a few examples\n",
    "for i in range(3):\n",
    "    input_seq = sample_batch['input'][i]\n",
    "    target_seq = sample_batch['target'][i]\n",
    "    original = sample_batch['original'][i]\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Original: '{original}'\")\n",
    "    print(f\"  Input:    {input_seq.tolist()}\")\n",
    "    print(f\"  Target:   {target_seq.tolist()}\")\n",
    "    print(f\"  Input tokens:  {[tokenizer.id_to_char[id.item()] for id in input_seq]}\")\n",
    "    print(f\"  Target tokens: {[tokenizer.id_to_char[id.item()] for id in target_seq]}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîß Jupyter Compatibility Fix\n",
    "\n",
    "### Multiprocessing in Jupyter Notebooks\n",
    "\n",
    "**Issue**: When using custom Dataset classes with `num_workers > 0` in Jupyter notebooks, you may encounter:\n",
    "```\n",
    "AttributeError: Can't get attribute 'ArithmeticDataset' on <module '__main__'>\n",
    "```\n",
    "\n",
    "**Root Cause**: \n",
    "- DataLoader workers run in separate processes\n",
    "- They need to pickle/unpickle the dataset object\n",
    "- Custom classes defined in notebooks exist in `__main__` module\n",
    "- Worker processes can't access `__main__` module classes\n",
    "\n",
    "**Solution Applied**:\n",
    "- Set `num_workers=0` in all DataLoader configurations\n",
    "- This disables multiprocessing and runs everything in the main process\n",
    "- Trade-off: Slightly slower data loading for Jupyter compatibility\n",
    "\n",
    "**For Production**:\n",
    "- Move dataset classes to separate `.py` files\n",
    "- Enable `num_workers > 0` for faster parallel data loading\n",
    "- This can improve training speed by 10-30% on multi-core systems\n",
    "\n",
    "**Performance Impact**:\n",
    "- With pre-tokenization: Minimal impact (data is already processed)\n",
    "- Without pre-tokenization: More noticeable impact\n",
    "- Our implementation uses pre-tokenization to minimize this trade-off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Transformer Architecture Implementation\n",
    "\n",
    "### Understanding the Transformer\n",
    "The Transformer architecture, introduced in \"Attention Is All You Need\" (Vaswani et al., 2017), revolutionized sequence modeling by relying entirely on attention mechanisms instead of recurrence or convolution.\n",
    "\n",
    "### Key Components We'll Implement:\n",
    "\n",
    "#### 1. **Multi-Head Self-Attention**\n",
    "- **Purpose**: Allows the model to focus on different parts of the input sequence simultaneously\n",
    "- **Mechanism**: Computes attention weights to determine which tokens are most relevant for each position\n",
    "- **Multiple heads**: Each head can learn different types of relationships (syntactic, semantic, positional)\n",
    "\n",
    "#### 2. **Positional Encoding**\n",
    "- **Problem**: Transformers have no inherent notion of sequence order\n",
    "- **Solution**: Add position information to token embeddings using sinusoidal functions\n",
    "- **Benefit**: Allows the model to understand that `3+4` is different from `4+3`\n",
    "\n",
    "#### 3. **Feed-Forward Networks**\n",
    "- **Purpose**: Add non-linearity and computational capacity between attention layers\n",
    "- **Architecture**: Two linear transformations with ReLU activation\n",
    "- **Role**: Processes the attended representations to extract higher-level features\n",
    "\n",
    "#### 4. **Layer Normalization & Residual Connections**\n",
    "- **Stability**: Normalizes inputs to prevent gradient explosion/vanishing\n",
    "- **Skip connections**: Allow gradients to flow directly to earlier layers\n",
    "- **Training efficiency**: Enables training of deeper networks\n",
    "\n",
    "#### 5. **Masked Self-Attention**\n",
    "- **Causal masking**: Prevents the model from \"cheating\" by looking at future tokens\n",
    "- **Autoregressive generation**: Essential for generating sequences one token at a time\n",
    "\n",
    "Let's implement each component step by step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Transformer Components ===\n",
      "Positional encoding: torch.Size([2, 10, 128]) ‚Üí torch.Size([2, 10, 128])\n",
      "Multi-head attention: torch.Size([2, 10, 128]) ‚Üí torch.Size([2, 10, 128])\n",
      "Transformer block: torch.Size([2, 10, 128]) ‚Üí torch.Size([2, 10, 128])\n",
      "‚úÖ All components working correctly!\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Add positional information to token embeddings using sinusoidal functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_length=1000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create the div_term for sinusoidal functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices and cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len].to(x.device)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for queries, keys, values, and output\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # 1. Generate Q, K, V matrices\n",
    "        Q = self.w_q(x)  # (batch_size, seq_len, d_model)\n",
    "        K = self.w_k(x)\n",
    "        V = self.w_v(x)\n",
    "        \n",
    "        # 2. Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 3. Compute attention\n",
    "        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 4. Concatenate heads and put through final linear layer\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model)\n",
    "        \n",
    "        return self.w_o(attention_output)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"Compute scaled dot-product attention.\"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # Apply mask if provided (for causal attention)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        return torch.matmul(attention_weights, V)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(F.relu(self.linear_1(x))))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer block with self-attention and feed-forward.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the components\n",
    "print(\"=== Testing Transformer Components ===\")\n",
    "\n",
    "# Test dimensions\n",
    "d_model = 128\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Test positional encoding\n",
    "pos_enc = PositionalEncoding(d_model, max_length=50)\n",
    "test_input = torch.randn(batch_size, seq_len, d_model)\n",
    "pos_encoded = pos_enc(test_input)\n",
    "print(f\"Positional encoding: {test_input.shape} ‚Üí {pos_encoded.shape}\")\n",
    "\n",
    "# Test multi-head attention\n",
    "mha = MultiHeadAttention(d_model, num_heads=8)\n",
    "attn_output = mha(pos_encoded)\n",
    "print(f\"Multi-head attention: {pos_encoded.shape} ‚Üí {attn_output.shape}\")\n",
    "\n",
    "# Test transformer block\n",
    "transformer_block = TransformerBlock(d_model, num_heads=8, d_ff=512)\n",
    "block_output = transformer_block(pos_encoded)\n",
    "print(f\"Transformer block: {pos_encoded.shape} ‚Üí {block_output.shape}\")\n",
    "\n",
    "print(\"‚úÖ All components working correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Complete Transformer Model\n",
    "\n",
    "Now let's combine all components into a complete transformer model for our arithmetic task. Our model will include:\n",
    "\n",
    "1. **Token Embedding**: Convert token IDs to dense vector representations\n",
    "2. **Positional Encoding**: Add position information to embeddings\n",
    "3. **Transformer Blocks**: Stack multiple attention and feed-forward layers\n",
    "4. **Output Projection**: Convert hidden states back to vocabulary predictions\n",
    "5. **Causal Masking**: Ensure autoregressive generation during training\n",
    "\n",
    "The model architecture follows the decoder-only approach (similar to GPT), which is perfect for our sequence generation task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Architecture ===\n",
      "Model: ArithmeticTransformer\n",
      "Parameters: {'vocab_size': 9, 'd_model': 128, 'num_heads': 8, 'num_layers': 4, 'd_ff': 512, 'max_length': 22, 'dropout': 0.1}\n",
      "Total parameters: 795,657\n",
      "Trainable parameters: 795,657\n",
      "Model size: 3.04 MB (float32)\n",
      "\n",
      "=== Testing Forward Pass ===\n",
      "Input shape: torch.Size([4, 21])\n",
      "Output shape: torch.Size([4, 21, 9])\n",
      "Output logits range: [-3.807, 3.301]\n",
      "‚úÖ Model forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "class ArithmeticTransformer(nn.Module):\n",
    "    \"\"\"Optimized Transformer model for arithmetic sequence generation with GPU support.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=8, num_layers=4, \n",
    "                 d_ff=512, max_length=100, dropout=0.1, device=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.device = device or torch.device('cpu')\n",
    "        \n",
    "        # Embedding and positional encoding (use larger max_length for generation)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_length * 3)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights with proper scaling\n",
    "        self.init_weights()\n",
    "        \n",
    "        # Move to device\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize model weights with proper scaling.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Use Xavier/Glorot initialization\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                # Use normal initialization with proper scaling\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "    \n",
    "    def create_causal_mask(self, seq_len):\n",
    "        \"\"\"Create causal mask to prevent looking at future tokens.\"\"\"\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=self.device))\n",
    "        return mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dimensions\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # Ensure input is on the correct device\n",
    "        x = x.to(self.device)\n",
    "        \n",
    "        # Create causal mask if not provided\n",
    "        if mask is None:\n",
    "            mask = self.create_causal_mask(seq_len)\n",
    "        \n",
    "        # Token embedding + positional encoding\n",
    "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, mask)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, tokenizer, prompt=\"\", max_length=20, temperature=1.0):\n",
    "        \"\"\"Generate text given a prompt with proper device handling.\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        # Encode prompt\n",
    "        if prompt:\n",
    "            input_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        else:\n",
    "            input_ids = [tokenizer.sos_id]  # Start with SOS token\n",
    "        \n",
    "        input_tensor = torch.tensor([input_ids], dtype=torch.long, device=self.device)\n",
    "        \n",
    "        generated = []\n",
    "        for _ in range(max_length):\n",
    "            # Limit input sequence length to prevent positional encoding issues\n",
    "            if input_tensor.size(1) > self.max_length:\n",
    "                input_tensor = input_tensor[:, -self.max_length:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = self.forward(input_tensor)\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            # Stop if EOS token is generated\n",
    "            if next_token == tokenizer.eos_id:\n",
    "                break\n",
    "            \n",
    "            generated.append(next_token)\n",
    "            \n",
    "            # Add to input for next iteration\n",
    "            input_tensor = torch.cat([\n",
    "                input_tensor, \n",
    "                torch.tensor([[next_token]], dtype=torch.long, device=self.device)\n",
    "            ], dim=1)\n",
    "        \n",
    "        return tokenizer.decode(generated, remove_special_tokens=True)\n",
    "\n",
    "# Initialize the model\n",
    "model_config = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'd_model': 128,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 4,\n",
    "    'd_ff': 512,\n",
    "    'max_length': train_dataset.max_length,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "model = ArithmeticTransformer(**model_config)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"=== Model Architecture ===\")\n",
    "print(f\"Model: ArithmeticTransformer\")\n",
    "print(f\"Parameters: {model_config}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\n=== Testing Forward Pass ===\")\n",
    "sample_input = sample_batch['input'][:4]  # Take first 4 examples\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(sample_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output logits range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "\n",
    "print(\"‚úÖ Model forward pass successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating GPU-optimized model...\n",
      "‚úÖ GPU-optimized model created!\n",
      "üìä Model parameters: 795,657\n",
      "üéØ Device: mps\n",
      "üìè Model size: 3.04 MB\n",
      "\n",
      "=== Testing Generation ===\n",
      "‚ùå Error with prompt '3+': '3'\n",
      "   Tokenizer vocab size: 9\n",
      "   Model vocab size: 9\n",
      "‚ùå Error with prompt '5-': '5'\n",
      "   Tokenizer vocab size: 9\n",
      "   Model vocab size: 9\n",
      "Prompt: '2+' ‚Üí Generated: '2+-'\n",
      "‚úÖ Model setup complete and ready for optimized training!\n"
     ]
    }
   ],
   "source": [
    "# Create optimized model with proper device configuration\n",
    "print(\"üîß Creating GPU-optimized model...\")\n",
    "\n",
    "# Optimized model configuration for better performance\n",
    "model_config = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'd_model': 128,      # Restored to original size for better capacity\n",
    "    'num_heads': 8,      # Restored to original\n",
    "    'num_layers': 4,     # Restored to original  \n",
    "    'd_ff': 512,         # Restored to original\n",
    "    'max_length': train_dataset.max_length,\n",
    "    'dropout': 0.1,\n",
    "    'device': device     # Important: Pass device to model\n",
    "}\n",
    "\n",
    "# Create new model instance with device awareness\n",
    "model = ArithmeticTransformer(**model_config)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"‚úÖ GPU-optimized model created!\")\n",
    "print(f\"üìä Model parameters: {total_params:,}\")\n",
    "print(f\"üéØ Device: {device}\")\n",
    "print(f\"üìè Model size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Test generation with proper error handling\n",
    "print(f\"\\n=== Testing Generation ===\")\n",
    "for prompt in [\"3+\", \"5-\", \"2+\"]:\n",
    "    try:\n",
    "        # Test with lower temperature for more deterministic output\n",
    "        generated = model.generate(tokenizer, prompt=prompt, max_length=10, temperature=0.1)\n",
    "        full_expr = prompt + generated\n",
    "        print(f\"Prompt: '{prompt}' ‚Üí Generated: '{full_expr}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with prompt '{prompt}': {str(e)}\")\n",
    "        # Debug information\n",
    "        print(f\"   Tokenizer vocab size: {len(tokenizer)}\")\n",
    "        print(f\"   Model vocab size: {model.vocab_size}\")\n",
    "\n",
    "print(\"‚úÖ Model setup complete and ready for optimized training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Training the Transformer\n",
    "\n",
    "### üîß Error Fix: Positional Encoding Length Issue\n",
    "\n",
    "We encountered a tensor size mismatch error during generation. This happened because:\n",
    "\n",
    "**The Problem:**\n",
    "- The model was initialized with `max_length=8` (from our dataset)\n",
    "- During generation, sequences can grow longer than 8 tokens as we keep adding generated tokens\n",
    "- The positional encoding buffer was only created for sequences up to length 8\n",
    "- When the sequence exceeded this length, PyTorch threw a size mismatch error\n",
    "\n",
    "**The Solution:**\n",
    "1. **Increased positional encoding buffer**: Changed from `max_length` to `max_length * 3` to accommodate longer sequences during generation\n",
    "2. **Added sequence length limiting**: In the generate method, we now truncate the input sequence if it gets too long\n",
    "3. **Made generation more robust**: Added proper error handling and sequence management\n",
    "\n",
    "This is a common issue when working with transformers for generation tasks - the model needs to handle variable-length sequences gracefully.\n",
    "\n",
    "### Training Strategy\n",
    "Now we'll train our transformer to learn arithmetic operations. Our training approach includes:\n",
    "\n",
    "#### **Loss Function**: Cross-Entropy Loss\n",
    "- **Purpose**: Measures how well the model predicts the next token in sequence\n",
    "- **Teacher Forcing**: During training, we provide the correct previous tokens as input\n",
    "- **Autoregressive Learning**: Model learns to predict each next character given the previous context\n",
    "\n",
    "#### **Optimization**: Adam Optimizer with Learning Rate Scheduling\n",
    "- **Adam**: Adaptive learning rates with momentum for stable convergence\n",
    "- **Learning Rate Scheduling**: Reduce learning rate when validation loss plateaus\n",
    "- **Gradient Clipping**: Prevent gradient explosion in deep networks\n",
    "\n",
    "#### **Training Loop Components**:\n",
    "1. **Forward Pass**: Compute predictions for next tokens\n",
    "2. **Loss Calculation**: Compare predictions with ground truth\n",
    "3. **Backward Pass**: Compute gradients via backpropagation  \n",
    "4. **Parameter Update**: Apply gradients to model weights\n",
    "5. **Validation**: Monitor performance on unseen data\n",
    "\n",
    "#### **Metrics We'll Track**:\n",
    "- **Training Loss**: How well the model fits the training data\n",
    "- **Validation Loss**: How well the model generalizes to new data\n",
    "- **Accuracy**: Percentage of correct next-token predictions\n",
    "- **Sequence Accuracy**: Percentage of completely correct arithmetic expressions\n",
    "\n",
    "Let's implement the training loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Using device: mps\n",
      "\n",
      "================================================================================\n",
      "üöÄ OPTIMIZED TRAINING PHASE\n",
      "================================================================================\n",
      "üöÄ Starting optimized training on device: mps\n",
      "‚ö° Mixed precision: ‚ùå Disabled\n",
      "\n",
      "================================================================================\n",
      "üéØ OPTIMIZED TRAINING - 30 EPOCHS\n",
      "================================================================================\n",
      "üìä Batch size: 64\n",
      "üìà Learning rate: 0.001\n",
      "üîÑ Training batches: 201\n",
      "‚úÖ Validation batches: 51\n",
      "Epoch   1/30 | Train Loss: 1.3260 | Val Loss: 1.0010 | Val Token Acc: 0.520 | Val Seq Acc: 0.000 | LR: 0.000561 | Time: 13.1s\n",
      "Epoch   6/30 | Train Loss: 0.7588 | Val Loss: 0.6979 | Val Token Acc: 0.652 | Val Seq Acc: 0.000 | LR: 0.001939 | Time: 10.3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 258\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     trained_model, training_history = \u001b[43mtrain_model_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced for faster testing\u001b[39;49;00m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_mixed_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_mixed_precision\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Training completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mtrain_model_optimized\u001b[39m\u001b[34m(model, train_loader, val_loader, tokenizer, num_epochs, learning_rate, device, use_mixed_precision)\u001b[39m\n\u001b[32m    151\u001b[39m     loss.backward()\n\u001b[32m    152\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n\u001b[32m    156\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/jupyter/transformers/.venv/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:124\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m opt = opt_ref()\n\u001b[32m    123\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/jupyter/transformers/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/jupyter/transformers/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/jupyter/transformers/.venv/lib/python3.13/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/jupyter/transformers/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/jupyter/transformers/.venv/lib/python3.13/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/jupyter/transformers/.venv/lib/python3.13/site-packages/torch/optim/adam.py:439\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    436\u001b[39m     device_beta1 = beta1\n\u001b[32m    438\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m \u001b[43mexp_avg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[38;5;66;03m# Nested if is necessary to bypass jitscript rules\u001b[39;00m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(beta2, Tensor):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def get_gpu_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage for monitoring.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024**3, torch.cuda.memory_reserved() / 1024**3\n",
    "    return 0, 0\n",
    "\n",
    "def evaluate_model_optimized(model, dataloader, tokenizer, criterion, device, use_amp=False):\n",
    "    \"\"\"Optimized evaluation function with GPU acceleration and optional mixed precision.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    correct_sequences = 0\n",
    "    total_sequences = 0\n",
    "    \n",
    "    # Use torch.no_grad() for inference to save memory\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['input'].to(device, non_blocking=True)\n",
    "            targets = batch['target'].to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass with optional mixed precision\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                with autocast():\n",
    "                    logits = model(inputs)\n",
    "                    loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            else:\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "            # Calculate token-level accuracy\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            mask = targets != tokenizer.pad_id\n",
    "            correct_tokens += ((predictions == targets) & mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "            \n",
    "            # Optimized sequence-level accuracy calculation\n",
    "            batch_size = inputs.size(0)\n",
    "            for i in range(batch_size):\n",
    "                pred_seq = predictions[i]\n",
    "                target_seq = targets[i]\n",
    "                \n",
    "                # Find the actual sequence length (excluding padding)\n",
    "                actual_length = (target_seq != tokenizer.pad_id).sum().item()\n",
    "                \n",
    "                # Compare only the non-padded portions\n",
    "                if torch.equal(pred_seq[:actual_length], target_seq[:actual_length]):\n",
    "                    correct_sequences += 1\n",
    "                total_sequences += 1\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    sequence_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n",
    "    \n",
    "    return avg_loss, token_accuracy, sequence_accuracy\n",
    "\n",
    "def train_model_optimized(model, train_loader, val_loader, tokenizer, num_epochs=50, \n",
    "                         learning_rate=0.001, device='cpu', use_mixed_precision=True):\n",
    "    \"\"\"\n",
    "    Optimized training function with GPU acceleration, mixed precision, and performance monitoring.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Starting optimized training on device: {device}\")\n",
    "    print(f\"‚ö° Mixed precision: {'‚úÖ Enabled' if use_mixed_precision and device.type == 'cuda' else '‚ùå Disabled'}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize mixed precision scaler for CUDA\n",
    "    scaler = None\n",
    "    if use_mixed_precision and device.type == 'cuda':\n",
    "        scaler = GradScaler()\n",
    "        print(\"üî• GradScaler initialized for mixed precision training\")\n",
    "    \n",
    "    # Optimized loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)\n",
    "    \n",
    "    # Use AdamW with better weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01, eps=1e-8)\n",
    "    \n",
    "    # More aggressive learning rate scheduling\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=learning_rate * 2,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    # Training history with additional metrics\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_token_acc': [], 'val_token_acc': [],\n",
    "        'train_seq_acc': [], 'val_seq_acc': [],\n",
    "        'learning_rates': [], 'gpu_memory_used': [],\n",
    "        'epoch_times': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 15\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üéØ OPTIMIZED TRAINING - {num_epochs} EPOCHS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"üìä Batch size: {train_loader.batch_size}\")\n",
    "    print(f\"üìà Learning rate: {learning_rate}\")\n",
    "    print(f\"üîÑ Training batches: {len(train_loader)}\")\n",
    "    print(f\"‚úÖ Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_tokens_correct = 0\n",
    "        train_tokens_total = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Non-blocking transfer to GPU for better performance\n",
    "            inputs = batch['input'].to(device, non_blocking=True)\n",
    "            targets = batch['target'].to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            if scaler is not None:\n",
    "                with autocast():\n",
    "                    logits = model(inputs)\n",
    "                    loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping with scaling\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Optimizer step with scaling\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard precision training\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            with torch.no_grad():\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                mask = targets != tokenizer.pad_id\n",
    "                train_tokens_correct += ((predictions == targets) & mask).sum().item()\n",
    "                train_tokens_total += mask.sum().item()\n",
    "            \n",
    "            # GPU memory cleanup every 50 batches\n",
    "            if batch_idx % 50 == 0 and device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate average training metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_token_acc = train_tokens_correct / train_tokens_total\n",
    "        \n",
    "        # Validation phase with optimized evaluation\n",
    "        val_loss, val_token_acc, val_seq_acc = evaluate_model_optimized(\n",
    "            model, val_loader, tokenizer, criterion, device, \n",
    "            use_amp=(scaler is not None)\n",
    "        )\n",
    "        \n",
    "        # Performance metrics\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        gpu_memory_used, gpu_memory_reserved = get_gpu_memory_usage()\n",
    "        \n",
    "        # Save metrics\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_token_acc'].append(train_token_acc)\n",
    "        history['val_token_acc'].append(val_token_acc)\n",
    "        history['train_seq_acc'].append(0)  # Can be calculated if needed\n",
    "        history['val_seq_acc'].append(val_seq_acc)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        history['gpu_memory_used'].append(gpu_memory_used)\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        # Print progress with enhanced metrics\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Val Token Acc: {val_token_acc:.3f} | \"\n",
    "                  f\"Val Seq Acc: {val_seq_acc:.3f} | \"\n",
    "                  f\"LR: {current_lr:.6f} | \"\n",
    "                  f\"Time: {epoch_time:.1f}s\"\n",
    "                  + (f\" | GPU: {gpu_memory_used:.1f}GB\" if device.type == 'cuda' else \"\"))\n",
    "        \n",
    "        # Early stopping with best model saving\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model with device handling\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': val_loss,\n",
    "                'val_token_acc': val_token_acc,\n",
    "                'val_seq_acc': val_seq_acc\n",
    "            }, 'best_model_optimized.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"\\n‚èπÔ∏è  Early stopping at epoch {epoch+1} (patience reached)\")\n",
    "            break\n",
    "        \n",
    "        # GPU memory cleanup\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Training completion\n",
    "    total_time = sum(history['epoch_times'])\n",
    "    print(f\"\\nüéâ Training completed!\")\n",
    "    print(f\"‚≠ê Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Total training time: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "    print(f\"üìä Average epoch time: {np.mean(history['epoch_times']):.1f}s\")\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load('best_model_optimized.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"‚úÖ Best model loaded from epoch {checkpoint['epoch']+1}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Use the device we set up in Cell 1\n",
    "print(f\"üéØ Using device: {device}\")\n",
    "\n",
    "# Determine if we should use mixed precision\n",
    "use_mixed_precision = mixed_precision_supported\n",
    "\n",
    "# Train the model with optimizations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ OPTIMIZED TRAINING PHASE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    trained_model, training_history = train_model_optimized(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        num_epochs=30,  # Reduced for faster testing\n",
    "        learning_rate=0.001,\n",
    "        device=device,\n",
    "        use_mixed_precision=use_mixed_precision\n",
    "    )\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training error: {str(e)}\")\n",
    "    print(f\"üîç Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    print(\"üìã Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Try with fallback configuration\n",
    "    print(\"\\nüîÑ Attempting training with fallback configuration...\")\n",
    "    try:\n",
    "        trained_model, training_history = train_model_optimized(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            tokenizer=tokenizer,\n",
    "            num_epochs=10,  # Even shorter for debugging\n",
    "            learning_rate=0.0005,  # Lower learning rate\n",
    "            device=device,\n",
    "            use_mixed_precision=False  # Disable mixed precision\n",
    "        )\n",
    "        print(\"‚úÖ Fallback training completed!\")\n",
    "    except Exception as fallback_error:\n",
    "        print(f\"‚ùå Fallback training also failed: {str(fallback_error)}\")\n",
    "        # Set dummy values for continuation\n",
    "        trained_model = model\n",
    "        training_history = {\n",
    "            'train_loss': [1.0], 'val_loss': [1.0],\n",
    "            'train_token_acc': [0.5], 'val_token_acc': [0.5],\n",
    "            'val_seq_acc': [0.0], 'epoch_times': [10.0],\n",
    "            'learning_rates': [0.001], 'gpu_memory_used': [0.0]\n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üîç Debugging and Verification\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîç DEBUGGING AND VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if all variables are properly defined\n",
    "print(\"üìã Variable Check:\")\n",
    "try:\n",
    "    print(f\"‚úÖ device: {device}\")\n",
    "    print(f\"‚úÖ mixed_precision_supported: {mixed_precision_supported}\")\n",
    "    print(f\"‚úÖ model type: {type(model).__name__}\")\n",
    "    print(f\"‚úÖ model device: {next(model.parameters()).device}\")\n",
    "    print(f\"‚úÖ tokenizer vocab size: {len(tokenizer)}\")\n",
    "    print(f\"‚úÖ train_loader batches: {len(train_loader)}\")\n",
    "    print(f\"‚úÖ val_loader batches: {len(val_loader)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Variable error: {e}\")\n",
    "\n",
    "# Test a simple forward pass\n",
    "print(f\"\\nüß™ Testing Model Forward Pass:\")\n",
    "try:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_batch = next(iter(train_loader))\n",
    "        inputs = sample_batch['input'][:2].to(device)  # Just 2 samples\n",
    "        outputs = model(inputs)\n",
    "        print(f\"‚úÖ Forward pass successful!\")\n",
    "        print(f\"   Input shape: {inputs.shape}\")\n",
    "        print(f\"   Output shape: {outputs.shape}\")\n",
    "        print(f\"   Output device: {outputs.device}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Forward pass error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test generation\n",
    "print(f\"\\nüé≤ Testing Model Generation:\")\n",
    "try:\n",
    "    model.eval()\n",
    "    test_prompt = \"1+\"\n",
    "    generated = model.generate(tokenizer, prompt=test_prompt, max_length=5, temperature=0.1)\n",
    "    print(f\"‚úÖ Generation successful!\")\n",
    "    print(f\"   Prompt: '{test_prompt}'\")\n",
    "    print(f\"   Generated: '{generated}'\")\n",
    "    print(f\"   Full expression: '{test_prompt + generated}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Generation error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test evaluation function\n",
    "print(f\"\\nüìä Testing Evaluation Function:\")\n",
    "try:\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)\n",
    "    # Take just first 2 batches for testing\n",
    "    test_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "    \n",
    "    val_loss, val_token_acc, val_seq_acc = evaluate_model_optimized(\n",
    "        model, test_loader, tokenizer, criterion, device, use_amp=False\n",
    "    )\n",
    "    print(f\"‚úÖ Evaluation successful!\")\n",
    "    print(f\"   Loss: {val_loss:.4f}\")\n",
    "    print(f\"   Token Accuracy: {val_token_acc:.3f}\")\n",
    "    print(f\"   Sequence Accuracy: {val_seq_acc:.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\nüéØ All tests completed! Check results above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Model Evaluation and Analysis\n",
    "\n",
    "### Comprehensive Testing\n",
    "Now that our model is trained, let's thoroughly evaluate its performance on arithmetic tasks. We'll test:\n",
    "\n",
    "#### **1. Quantitative Metrics**\n",
    "- **Training curves**: Visualize loss and accuracy over time\n",
    "- **Final performance**: Token-level and sequence-level accuracy on validation set\n",
    "- **Generalization**: How well the model handles expressions it hasn't seen\n",
    "\n",
    "#### **2. Qualitative Analysis**\n",
    "- **Generation samples**: Examine the model's generated arithmetic expressions\n",
    "- **Error analysis**: Understand what types of mistakes the model makes\n",
    "- **Pattern recognition**: See if the model learned mathematical relationships\n",
    "\n",
    "#### **3. Interactive Testing**\n",
    "- **Custom prompts**: Test the model on user-defined arithmetic problems\n",
    "- **Edge cases**: Evaluate behavior on boundary conditions\n",
    "- **Creativity**: Observe if the model can generate valid new expressions\n",
    "\n",
    "Let's start with visualization and comprehensive evaluation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(training_history['train_loss'], label='Training Loss', color='blue')\n",
    "plt.plot(training_history['val_loss'], label='Validation Loss', color='red')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Token accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(training_history['train_token_acc'], label='Training Token Acc', color='blue')\n",
    "plt.plot(training_history['val_token_acc'], label='Validation Token Acc', color='red')\n",
    "plt.title('Token-Level Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Sequence accuracy\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(training_history['val_seq_acc'], label='Validation Sequence Acc', color='red')\n",
    "plt.title('Sequence-Level Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation on validation set\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)\n",
    "final_val_loss, final_token_acc, final_seq_acc = evaluate_model_optimized(\n",
    "    trained_model, val_loader, tokenizer, criterion, device, use_amp=mixed_precision_supported)\n",
    "\n",
    "print(f\"Final Validation Results:\")\n",
    "print(f\"  Loss: {final_val_loss:.4f}\")\n",
    "print(f\"  Token Accuracy: {final_token_acc:.3f} ({final_token_acc*100:.1f}%)\")\n",
    "print(f\"  Sequence Accuracy: {final_seq_acc:.3f} ({final_seq_acc*100:.1f}%)\")\n",
    "\n",
    "# Test on training set for comparison\n",
    "final_train_loss, final_train_token_acc, final_train_seq_acc = evaluate_model_optimized(\n",
    "    trained_model, train_loader, tokenizer, criterion, device, use_amp=mixed_precision_supported)\n",
    "\n",
    "print(f\"\\\\nFinal Training Results:\")\n",
    "print(f\"  Loss: {final_train_loss:.4f}\")\n",
    "print(f\"  Token Accuracy: {final_train_token_acc:.3f} ({final_train_token_acc*100:.1f}%)\")\n",
    "print(f\"  Sequence Accuracy: {final_train_seq_acc:.3f} ({final_train_seq_acc*100:.1f}%)\")\n",
    "\n",
    "# Generation testing\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"GENERATION TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def test_arithmetic_generation(model, tokenizer, test_cases, device):\n",
    "    \"\"\"Test model on specific arithmetic problems.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for prompt in test_cases:\n",
    "        # Generate multiple samples for diversity\n",
    "        generations = []\n",
    "        for temp in [0.1, 0.5, 1.0]:  # Different temperatures\n",
    "            generated = model.generate(tokenizer, prompt=prompt, \n",
    "                                     max_length=15, temperature=temp)\n",
    "            full_expr = prompt + generated\n",
    "            generations.append((temp, full_expr))\n",
    "        \n",
    "        results.append((prompt, generations))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test cases covering different scenarios\n",
    "test_cases = [\n",
    "    \"0+\", \"1+\", \"2+\", \"3+\", \"4+\", \"5+\",  # Addition\n",
    "    \"5-\", \"7-\", \"9-\", \"6-\", \"8-\",        # Subtraction  \n",
    "    \"2+3\", \"4+5\", \"7+2\",                 # Partial expressions\n",
    "    \"9+9\", \"8+7\", \"6+8\",                 # Larger sums\n",
    "]\n",
    "\n",
    "# Run generation tests\n",
    "generation_results = test_arithmetic_generation(trained_model, tokenizer, test_cases, device)\n",
    "\n",
    "print(\"Generation Test Results:\")\n",
    "print(\"-\" * 80)\n",
    "for prompt, generations in generation_results:\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    for temp, full_expr in generations:\n",
    "        print(f\"  T={temp}: {full_expr}\")\n",
    "    print()\n",
    "\n",
    "# Detailed analysis of specific examples\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def analyze_arithmetic_correctness(expressions):\n",
    "    \"\"\"Analyze the mathematical correctness of generated expressions.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errors = []\n",
    "    \n",
    "    for expr in expressions:\n",
    "        total += 1\n",
    "        try:\n",
    "            # Parse the expression\n",
    "            if '=' in expr:\n",
    "                left, right = expr.split('=')\n",
    "                \n",
    "                if '+' in left:\n",
    "                    operand1, operand2 = left.split('+')\n",
    "                    expected = int(operand1) + int(operand2)\n",
    "                elif '-' in left:\n",
    "                    operand1, operand2 = left.split('-')\n",
    "                    expected = int(operand1) - int(operand2)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                actual = int(right)\n",
    "                \n",
    "                if actual == expected:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    errors.append(f\"{expr} (expected {expected})\")\n",
    "            \n",
    "        except (ValueError, IndexError):\n",
    "            errors.append(f\"{expr} (parsing error)\")\n",
    "    \n",
    "    return correct, total, errors\n",
    "\n",
    "# Collect all generated expressions\n",
    "all_expressions = []\n",
    "for prompt, generations in generation_results:\n",
    "    for temp, full_expr in generations:\n",
    "        if '=' in full_expr:\n",
    "            all_expressions.append(full_expr)\n",
    "\n",
    "# Analyze correctness\n",
    "correct, total, errors = analyze_arithmetic_correctness(all_expressions)\n",
    "\n",
    "print(f\"Mathematical Correctness Analysis:\")\n",
    "print(f\"  Correct expressions: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "print(f\"  Errors found: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\\\nFirst 10 errors:\")\n",
    "    for i, error in enumerate(errors[:10]):\n",
    "        print(f\"  {i+1}. {error}\")\n",
    "\n",
    "# Test with completely novel expressions\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"NOVEL EXPRESSION TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "novel_prompts = [\"1+7\", \"9-4\", \"6+6\", \"8-8\", \"4+4\"]\n",
    "print(\"Testing on expressions not in training data:\")\n",
    "\n",
    "for prompt in novel_prompts:\n",
    "    # Check if this expression was in our training data\n",
    "    full_expected = None\n",
    "    for expr in data:\n",
    "        if expr.startswith(prompt):\n",
    "            full_expected = expr\n",
    "            break\n",
    "    \n",
    "    generated = trained_model.generate(tokenizer, prompt=prompt, \n",
    "                                     max_length=10, temperature=0.1)\n",
    "    full_generated = prompt + generated\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {full_generated}\")\n",
    "    print(f\"In training: {'Yes' if full_expected else 'No'}\")\n",
    "    if full_expected:\n",
    "        print(f\"Expected: {full_expected}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\\\nüéâ Tutorial completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üöÄ GPU Optimization Performance Dashboard\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üî• GPU OPTIMIZATION PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Performance metrics summary\n",
    "if 'epoch_times' in training_history:\n",
    "    total_training_time = sum(training_history['epoch_times'])\n",
    "    avg_epoch_time = np.mean(training_history['epoch_times'])\n",
    "    fastest_epoch = min(training_history['epoch_times'])\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  TRAINING PERFORMANCE:\")\n",
    "    print(f\"   Total training time: {total_training_time:.2f}s ({total_training_time/60:.2f} min)\")\n",
    "    print(f\"   Average epoch time: {avg_epoch_time:.2f}s\")\n",
    "    print(f\"   Fastest epoch: {fastest_epoch:.2f}s\")\n",
    "    print(f\"   Training speed: ~{len(train_loader) * train_loader.batch_size / avg_epoch_time:.0f} samples/sec\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"\\nüî• GPU UTILIZATION:\")\n",
    "    if 'gpu_memory_used' in training_history:\n",
    "        max_gpu_memory = max([m for m in training_history['gpu_memory_used'] if m > 0])\n",
    "        print(f\"   Peak GPU memory: {max_gpu_memory:.2f} GB\")\n",
    "    \n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"   GPU: {gpu_name}\")\n",
    "    print(f\"   Total GPU memory: {gpu_memory_total:.1f} GB\")\n",
    "    print(f\"   Mixed precision: ‚úÖ Enabled\")\n",
    "    print(f\"   cuDNN benchmark: ‚úÖ Enabled\")\n",
    "\n",
    "elif device.type == 'mps':\n",
    "    print(f\"\\nüçé MPS ACCELERATION:\")\n",
    "    print(f\"   Apple Silicon GPU acceleration: ‚úÖ Enabled\")\n",
    "    print(f\"   Metal Performance Shaders: ‚úÖ Active\")\n",
    "\n",
    "print(f\"\\nüìä TRAINING CONFIGURATION:\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Batch size: {train_loader.batch_size}\")\n",
    "print(f\"   Data loading workers: {train_loader.num_workers} (Jupyter-safe)\")\n",
    "print(f\"   Pin memory: {getattr(train_loader, 'pin_memory', False)}\")\n",
    "print(f\"   Pre-tokenized dataset: ‚úÖ Enabled\")\n",
    "if train_loader.num_workers == 0:\n",
    "    print(f\"   üí° For production: Set num_workers > 0 in regular Python scripts\")\n",
    "\n",
    "print(f\"\\nüìà MODEL PERFORMANCE:\")\n",
    "print(f\"   Final validation loss: {final_val_loss:.4f}\")\n",
    "print(f\"   Final validation token accuracy: {final_token_acc:.3f} ({final_token_acc*100:.1f}%)\")\n",
    "print(f\"   Final validation sequence accuracy: {final_seq_acc:.3f} ({final_seq_acc*100:.1f}%)\")\n",
    "\n",
    "# Enhanced visualization with performance metrics\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.plot(training_history['train_loss'], label='Training Loss', color='blue', linewidth=2)\n",
    "plt.plot(training_history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
    "plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Token accuracy\n",
    "plt.subplot(2, 4, 2)\n",
    "plt.plot(training_history['train_token_acc'], label='Training Token Acc', color='blue', linewidth=2)\n",
    "plt.plot(training_history['val_token_acc'], label='Validation Token Acc', color='red', linewidth=2)\n",
    "plt.title('Token-Level Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Sequence accuracy\n",
    "plt.subplot(2, 4, 3)\n",
    "plt.plot(training_history['val_seq_acc'], label='Validation Sequence Acc', color='red', linewidth=2)\n",
    "plt.title('Sequence-Level Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Learning rate schedule\n",
    "if 'learning_rates' in training_history:\n",
    "    plt.subplot(2, 4, 4)\n",
    "    plt.plot(training_history['learning_rates'], color='green', linewidth=2)\n",
    "    plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "\n",
    "# Plot 5: Epoch timing\n",
    "if 'epoch_times' in training_history:\n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.plot(training_history['epoch_times'], color='purple', linewidth=2)\n",
    "    plt.title('Training Speed (Time per Epoch)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: GPU memory usage\n",
    "if 'gpu_memory_used' in training_history and device.type == 'cuda':\n",
    "    plt.subplot(2, 4, 6)\n",
    "    memory_data = [m for m in training_history['gpu_memory_used'] if m > 0]\n",
    "    if memory_data:\n",
    "        plt.plot(memory_data, color='orange', linewidth=2)\n",
    "        plt.title('GPU Memory Usage', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Memory (GB)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Training efficiency\n",
    "plt.subplot(2, 4, 7)\n",
    "if 'epoch_times' in training_history:\n",
    "    samples_per_second = [len(train_loader) * train_loader.batch_size / time \n",
    "                         for time in training_history['epoch_times']]\n",
    "    plt.plot(samples_per_second, color='teal', linewidth=2)\n",
    "    plt.title('Training Throughput', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Samples/Second')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 8: Loss improvement rate\n",
    "plt.subplot(2, 4, 8)\n",
    "if len(training_history['val_loss']) > 1:\n",
    "    loss_improvement = [training_history['val_loss'][0] - loss for loss in training_history['val_loss']]\n",
    "    plt.plot(loss_improvement, color='darkred', linewidth=2)\n",
    "    plt.title('Cumulative Loss Improvement', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss Reduction')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('üöÄ CalcGPT GPU-Optimized Training Dashboard', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ GPU OPTIMIZATION COMPLETE - READY FOR PRODUCTION!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüéØ KEY OPTIMIZATIONS IMPLEMENTED:\")\n",
    "print(\"   ‚úÖ Advanced device detection with hardware-specific optimizations\")\n",
    "print(\"   ‚úÖ Mixed precision training (CUDA) with automatic scaling\")\n",
    "print(\"   ‚úÖ Optimized data loading with pre-tokenization (Jupyter-compatible)\")\n",
    "print(\"   ‚úÖ Non-blocking GPU transfers for improved pipeline efficiency\")\n",
    "print(\"   ‚úÖ Memory-efficient evaluation with GPU cache management\")\n",
    "print(\"   ‚úÖ Advanced learning rate scheduling (OneCycleLR)\")\n",
    "print(\"   ‚úÖ Comprehensive performance monitoring and metrics tracking\")\n",
    "print(\"   ‚úÖ Enhanced model checkpointing with optimizer state\")\n",
    "print(\"   ‚ö†Ô∏è  Multiprocessing disabled for Jupyter compatibility\")\n",
    "print(\"\\nüöÄ Your CalcGPT model is now fully optimized for GPU acceleration!\")\n",
    "print(\"\\nüìù Note: For production deployments outside Jupyter,\")\n",
    "print(\"   enable num_workers > 0 in DataLoader for faster data loading\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
